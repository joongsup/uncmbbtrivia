<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jay&#39;s Notes</title>
    <link>/</link>
    <description>Recent content on Jay&#39;s Notes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 06 Oct 2018 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Vim, vim-slime, and screen</title>
      <link>/post/2018/10/06/vim-and-slime/</link>
      <pubDate>Sat, 06 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/10/06/vim-and-slime/</guid>
      <description>
        &lt;p&gt;As much as I love using RStudio for everything&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, there are times when I can’t really use it, i.e., for some projects at work. When that happens, it means I usually need to log in to some servers and do work there (i.e., on the server side), and at the moment, there is no way I can use RStudio for my work&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. It’s more than usual, in fact, and for a long time, I’ve been trying to establish a good/working data analysis workflow that I can follow in that setting.&lt;/p&gt;
&lt;p&gt;Vim is my server side editor of choice. I’m not a Vim expert by no means, but ever since I started using it (it’s been almost 4 years), it fit me, and it’s been my editor of choice, especially when working on some remote servers. I would open up several terminal window tabs, i.e. one for command line, one for R session, one for file editing (using Vim), one for hive shell, etc..Establishing a good/working data analysis workflow came down to being able to use Vim more seamlessly with those other tools that I use everyday.&lt;/p&gt;
&lt;p&gt;After several days of browsing, reading, and trying out, I finally settled down with this particular set-up and so far I’m pretty happy with it. I’m writing this post as a reference for myself and potentially others who’s facing a similar situation. This is one of those things that I wish I had perfected while I was still at school, just like many other things that I’ve written about before here.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Vim and .vimrc&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I searched web for information regarding Vim set up for data analysis, especially using R. Very quickly, I ran into &lt;a href=&#34;https://github.com/jalvesaq/Nvim-R&#34;&gt;Nvim-R&lt;/a&gt;, a Vim plugin to edit R code. I tried it for a couple of days, but ended up dropping it, because as much as I love R, I had to use some other tools with Vim, and I needed more general solution, not an R-specific solution.&lt;/p&gt;
&lt;p&gt;But that turned out to be my introduction to the world of Vim plugins! I spent a couple of nights searching for cool Vim plugins, sometimes for the ones that are not necessarily related to doing data analysis. But it was so fun trying out a variety of Vim plugins. This is also when I started using Vim plugin manager as well, which makes installing/uninstalling Vim plugins simple and easy. I tried only one such manager, &lt;a href=&#34;https://github.com/VundleVim/Vundle.vim&#34;&gt;Vundle&lt;/a&gt;, and have been using it ever since.&lt;/p&gt;
&lt;p&gt;Now to use a Vim plugin manager and several plugins, I started adding more stuffs in my .vimrc file, which is like a .Rprofile file (or .bashrc). It contains many settings for Vim, i.e., plugins related specific setting as well as more general settings. And boy are there tons of cool stuffs you can do with these settings! Of the many general settings, one of my favorite is insert mode mapping of “jj” (two strokes of the key j) to esc key, so that I don’t have to reach for the esc key while typing, and instead just type jj to get back to normal mode&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. Like I said, I spent several couple of nights trying out different settings and plugins, and I had to resist the urge to play with new settings and plugins. It’s like trying to perfect how your blog looks like, and I had to try hard not to spend too much time perfecting my .vimrc file, i.e., my Vim setup.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Vim-slime&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then I ran into &lt;a href=&#34;https://github.com/jpalardy/vim-slime&#34;&gt;Vim-slime&lt;/a&gt;, another Vim plugin that allows you to send lines of code from one window to another, thus eliminating the need for constant copying from a text file (e.g., data analysis script file) and pasting on R session, for example. It’s the equivalence of selection and cmd-enter in RStudio to run select lines of code.&lt;/p&gt;
&lt;p&gt;With Vim-slime, I can send select lines of codes from one terminal window tab to another, be it an R console, Python console, Hive shell, etc. So it provided me with a more general solution than Nvim-R, which is still a great R specific plugin.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Screen&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Last but not least, I had to come up with a way to keep my running terminal sessions even after server connection is lost. I have several meetings a day, and sometimes I lose my server connection while walking to/from a meeting. I already knew and have been using &lt;a href=&#34;https://www.gnu.org/software/screen/&#34;&gt;screen&lt;/a&gt; for that purpose, and it so happens that Vim-slime works great with screen. By default, a chunk of code I want to run is sent to a screen session, and I learned how to name a screen session too.&lt;/p&gt;
&lt;p&gt;I also ran into &lt;a href=&#34;https://github.com/tmux/tmux&#34;&gt;tmux&lt;/a&gt;, which is a screen multiplexer, similar to GNU screen. Essentially just like screen, it lets you work on multiple tabs within one terminal window without losing them even after your server connection is lost. It looked very interesting and useful, but unfortunately, it needed to be installed on the server side, and to make things worse, the server was missing one of the required dependencies, so I decided to drop tmux from my target list.&lt;/p&gt;
&lt;p&gt;So putting these together, here’s what I typically do first thing in the morning at work. There’s probably a better way to do this still, but until I figure out how to, I’m happy with the current set up.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Open terminal and ssh into a server. Open the script file, e.g., an R script, that I need to work on. Let’s call this a “source” tab.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:source&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/slime_src.png&#34; alt=&#34;Source screenshot&#34; width=&#34;75%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Source screenshot
&lt;/p&gt;
&lt;/div&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Open another tab in the same terminal window (cmd-T) and ssh into the same server. Start a named screen session by &lt;code&gt;screen -S R&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;$screen -S R&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will open a screen sesion whose name is “R”. Once in it, start an R shell, (or python, hive, etc.), and let’s call it a “console” tab. I find it helpful to name each of these console tabs with corresponding tool name, e.g., “hive” when using hive shell there, “python” when using python shell there, and “R” when using R shell there, etc..&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;When working on some code chunks, test them by sending the code chunk from the “source” tab (from #1) to the “console” tab (from #2). Selecting code chunks in visual mode in Vim and sending them to the console screen by &lt;ctrl-c&gt;&lt;ctrl-c&gt; using Vim-slime takes some getting used to, but it’s been worth it :)&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:workflow&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/out.gif&#34; alt=&#34;Vim-slime workflow screenshots&#34; width=&#34;75%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Vim-slime workflow screenshots
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;When code chunks are sent for the first time in a session, Vim-slime asks for the destination screen session name, and I entered R, which was my screen session name from step 2 above. It then asks for screen window name, and I just leave it as is (default 0).&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Especially for blogging, yeah!&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Using RStudio server would do it, but it’s not an option at the moment&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;I wonder if there’s a way to keep using those preference in RStudio vim mode&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

        
      </description>
    </item>
    
    <item>
      <title>GNU Make for Data Analysis Workflow Management</title>
      <link>/post/2018/08/26/gnu-make-for-workflow-manager/</link>
      <pubDate>Sun, 26 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/08/26/gnu-make-for-workflow-manager/</guid>
      <description>
        &lt;p&gt;I’ve finally started using GNU &lt;a href=&#34;https://www.gnu.org/software/make/&#34;&gt;make&lt;/a&gt; as a data analysis workflow management tool. I knew it existed as a software “build” tool, and although I always thought Makefile&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; sound pretty cool, I never actually had to use it, not just as a build tool, but also as a data analysis workflow management tool.&lt;/p&gt;
&lt;p&gt;It started with &lt;span class=&#34;citation&#34;&gt;@thosjleepr&lt;/span&gt;’s &lt;a href=&#34;https://twitter.com/thosjleeper/status/1030105885085970432?s=03&#34;&gt;tweet&lt;/a&gt; that showed up in my feed:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Inspired by &lt;span class=&#34;citation&#34;&gt;@carlislerainey&lt;/span&gt;, I’ve added some code to my intro #make tutorial that visualizes the makefile’s dependency graph natively in R using #ggraph.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I never actually looked closely at what the author did in this nice tutorial, but rather ended up taking a detour and actually started looking more closely into using make as a workflow management tool. It so happened that at the time, I needed to keep running some set of scripts over and over depending on “refresh” status of each step, and so when I ran into this tweet, I decided to jump on using make finally, and now make is one of those topics that I wish I had learned and started using while at school&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It’s been less than 2 weeks since I started using make, and I’m not a make expert by no means. Nonetheless, &lt;a href=&#34;https://joongsup.rbind.io/about/&#34;&gt;I like talking about data anlaysis workflow&lt;/a&gt;, so here’s my thoughts on using make as a data analysis workflow management tool as a reference. Since it’s my thoughts as of today, I’m sure some of my takes will change over time.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;It’s relevant to many data analysis projects.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;From the Wikipedia on &lt;a href=&#34;https://en.wikipedia.org/wiki/Make_(software)&#34;&gt;make (software)&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Besides building programs, Make can be used to manage any project where some files must be updated automatically from others whenever the others change.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It makes sense many tutorials on make (especially in the context of using it together with R) that I encountered used generating some documents as a use case. E.g., use make to streamline updating plots and inserting newly refreshed plots in the final document output. My immediate need didn’t involve updating a report over and over, but rather looked something like Figure &lt;a href=&#34;#fig:workflow&#34;&gt;1&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:workflow&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/makefile_workflow.png&#34; alt=&#34;Base Workflow&#34; width=&#34;40%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Base Workflow
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Basically this example workflow involves generating/refreshing several hive tables, training ML models, and joining hive tables. Sometimes all 5 steps need to be run in a proper sequence, other times only a subset of steps need to be run, like Figure &lt;a href=&#34;#fig:scenarios&#34;&gt;2&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:scenarios&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/makefile_ex_1.png&#34; alt=&#34;Workflow Scenarios&#34; width=&#34;40%&#34; /&gt;&lt;img src=&#34;/img/makefile_ex_3.png&#34; alt=&#34;Workflow Scenarios&#34; width=&#34;40%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Workflow Scenarios
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;So if there’s anyone out there who’s on the edge and haven’t started using a tool like make for workflow management, thinking such tool is not relevant in data analysis, I hope above figures (and this post) provide some convincing argument for using it.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;It helped me put more efforts in modularizing codes.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A Makefile is a collection of one or more rules, with a single rule consisting of a target, dependencies, and commands. In order to use commands in a Makefile, it’s imperative that the codes can be run in command line, and not just in interactive environment (REPL).&lt;/p&gt;
&lt;p&gt;I don’t know what’s the best way to write a “program” for a data analysis project, and many of my codes are still mostly used in interactive mode. With the use of make, however, I’m putting more efforts to make sure my codes are run in command line, and REPL is used only for checking snippet of codes, not the entirety. So instead of working exclusively in a REPL shell (say R and/or hive shell), I now try to make sure the script files work by starting them in command line.&lt;/p&gt;
&lt;p&gt;This is an important shift, I think, that has more implications, and I’m sure I’ll have more to say about this shift in the future.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;It helps keep track of workflow dependency and documentation in one place.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is a nice little benefit of using make that I liked nonetheless. When I work on a project for 2-3 days, take a break from it, and come back in 2-3 weeks, I want to be able to recall what I have done and start picking up on things with minimal effort. With a combination of comments and a sed command in a Makefile, helpful documentation/notes can be extracted from command line like below.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;~$ make help
 1_gen_tbl_A: generate/refresh table A
 2_train_model: train ml model using table A and generate table B
 3_gen_tbl_C: generate/refresh table C (not dependent on steps 1 and 2!)
 4_join_B_and_C: join tables B and C as a prep for step 5
 5_compare_B_and_C: compare B and C by creating a performance table
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Software carpentry’s &lt;a href=&#34;https://swcarpentry.github.io/make-novice/08-self-doc/index.html&#34;&gt;lesson&lt;/a&gt; on automation and make provides a how-to.&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;There are many alternatives to GNU make.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The tool has been around for nearly 40 years now. Obviously, I like it for several reasons as a workflow management tool, but at the same time, there are some limitations that can make it less attractive nowadays. I have not done a thorough research on this, but there seem to be many alternatives, each one of which has its own strength and weakness.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;R &lt;a href=&#34;https://github.com/ropensci/drake&#34;&gt;drake&lt;/a&gt; package is more R-focused than other pipeline tools, but more importantly, it has a very thorough and informative documentation on such tools in general.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Apache &lt;a href=&#34;https://airflow.apache.org/&#34;&gt;airflow&lt;/a&gt; is “a platform to programmatically author, schedule and monitor workflows.” I really like its ability to visualize dependencies.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Bash script: many data engineers that I know seem to be using bash scripts to do everything, including workflow mangement. It’s certainly a viable option, even when dependencies are considered, especially when you know what you’re doing.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are some great tutorials and documentations on GNU make, and here are a couple more links as a reference.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;GNU Make &lt;a href=&#34;https://www.gnu.org/software/make/manual/make.html&#34;&gt;Manual&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Make Intro by &lt;a href=&#34;http://kbroman.org/minimal_make/&#34;&gt;Karl Broman&lt;/a&gt;, &lt;a href=&#34;https://bost.ocks.org/mike/make/&#34;&gt;Mike Bostock&lt;/a&gt;, &lt;a href=&#34;http://zmjones.com/make/&#34;&gt;Zachary Jones&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Software Carpentry’s &lt;a href=&#34;https://swcarpentry.github.io/make-novice/&#34;&gt;Lesson&lt;/a&gt; on Automation and Make&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Make reads this Makefile (in the current directory) and works on the rules within it.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;I should prob start gathering these topics and write about them sometime in the future.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

        
      </description>
    </item>
    
    <item>
      <title>Home sweet dome</title>
      <link>/post/2018/08/05/home-sweet-dome/</link>
      <pubDate>Sun, 05 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/08/05/home-sweet-dome/</guid>
      <description>
        &lt;p&gt;I ran into a &lt;a href=&#34;https://www.tarheelblog.com/2018/8/4/17644436/unc-basketball-tar-heels-dean-dome-advantage&#34;&gt;post&lt;/a&gt; on the Tar Heel Blog (THB) that talks about the Tar Heel’s home court advantage in recent years. Since it’s been a while since I wrote anything about UNCMBB, I thought it’d be a great topic to write on here too, looking at how great the teams have played on home court&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; in recent years. And maybe I’ll look at how Duke has played on their home court too during the same time period just because.&lt;/p&gt;
&lt;p&gt;THB counted the wins and losses in the past 3 years in particular, and that gave me a starting point.&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Season
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Where
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
wins
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
losses
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2016
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
H
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
H
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
H
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;So it seems my count and their count matches. Then I was curious if this last 3 years have been the best there is, in terms of fewest losses, and for that matter, most wins, and best winning percentage at home. In below charts, season is the starting season of the 3 years. E.g., for season = 2016, value = 4 means, rolling 3 years loss count for 2016, 2017, and 2018 seasons. Championship seasons are represented with larger dots, and the latest 3 year statistics (wins, losses, and winning percentages) with coloured horizontal lines.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-05-home-sweet-dome_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Although this past 3 years loss count (4) is a great feat, it’s not the best there is, it turns out, and there’s been 18 instances with fewer than 4 loss count. Three early championship seasons (1957, 1982, and 1983) stand out with 3 or fewer losses each (again, as the starting season of the next 3 seasons), with the next two championship seasons (2005 and 2009) not so superb loss counts at 6 apiece&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The fewest 3-yr rolling home loss counts came in two separate instances, with a single lose apiece: 1977/1978/1979 seasons and 1978/1979/1980 seasons. Let’s see what those losses were.&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Season
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Game_Date
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Game_Day
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Type
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Where
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Opponent_School
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Result
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Tm
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Opp
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
OT
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1977
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1977-01-26
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Wed
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
REG
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
H
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Wake Forest
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
L
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
66
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
67
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1980
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1980-01-20
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Sun
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
REG
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
H
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Maryland
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
L
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
86
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
92
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Nothing stands out at me right away about Carolina’s 1977 - 1980 teams&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;, let alone 1977 Wake Forest and 1980 Maryland, but it must have been fantastic home court winning streaks for the Tar Heels. Below shows the 3-yr rolling win counts.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-05-home-sweet-dome_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Sure, 1977 to 1979 seasons show great home winning trends at the time (e.g., 35 home wins during 3 seasons starting from 1978 season was the most in UNC history&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; at the time), but the number of games played are somewhat different year after year, so let’s look at the winning percentage instead.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-05-home-sweet-dome_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Yet again, 3 years following 1977 and 1978 seasons showed the highest rolling 3-yr home winning percentages, so it must have been really fun watching Carolina playing and winning at home those days. I feel like I have some homework to do, getting to know them a little better.&lt;/p&gt;
&lt;p&gt;Now that I’ve looked at a couple of statistics for rolling 3 years, I became curious which freshman class has the bragging rights in terms of fewest losses, most wins, and highest winning percentage over their college carrers at home court. This should be pretty straightforward since a typical college career lasts for 4 years, which means all I have to change is the number of rolling years from 3 to 4. Below charts show the rolling 4-yr losses, wins, and win percentages, respectively.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-05-home-sweet-dome_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-05-home-sweet-dome_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-05-home-sweet-dome_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For Carolina, it seems the bragging rights go to the freshman classes in the late 70s whereas for Duke, they could go to any of the teams in the late 80s, early and late 2000s. I had expected Psycho-T’s 2005 freshman class to stand out in any of the three numbers, but to my surprise the class wasn’t the top although they were close especially in terms of rolling 4-yr wins. That got me thinking, maybe they did much better in away games (after all, that class never lost to Duke on their home court during their college careers), and that’s what I’ll be looking at next time.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Which, by the way, is deservingly going to be named after Coach Roy Williams.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Most recent championship season (2017) does not have 3-yr rolling counts yet.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Just a casual fan here :)&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;Since 1949-1950 season to be exact.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

        
      </description>
    </item>
    
    <item>
      <title>My old coding products</title>
      <link>/post/2018/07/27/my-old-coding-products/</link>
      <pubDate>Fri, 27 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/07/27/my-old-coding-products/</guid>
      <description>
        &lt;p&gt;Lately I’ve been thinking about why I care much about everything R and sharing the joy of using R, which deserves its own post. Much of it has to do with how I did and did not get a proper training in coding suitable for data analysis in the past, but as I looked back on my personal coding history, I came across hundreds of code files that I wrote in the past&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. Suffice to say, they were pretty important files in my education history, but I have totally forgotten about them until now.&lt;/p&gt;
&lt;p&gt;They are .m files, meant to be used in Matlab. As I started opening and reading a couple of those files, I had mixed feelings, ranging from agony (“omg, this code was from that stressful point in time”) and embarrassment (“omg, look at the logic and style I was using there”), to delight (“wow I’m defining several functions to be used in main script!”) and motivation (“Yup, I was this bad, and that’s exactly why I care about education in coding (especially R) for data analysis!”).&lt;/p&gt;
&lt;p&gt;It was quite refreshing to see how I used to code in Matlab, which had some similarities with R, hence giving me some perspectives on how my data analysis coding skills might have changed. Here are some observations of the past, in conjunction with present whenever applicable:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Since the purpose of the code files (“program”) was to show my particular engineering method works as intended, the codes were relatively simple having simluated data as its input data. There was no cleaning/exploring step, whereas now, where almost always, there’s no clean data, thus always having to write cleaning/exploring scripts, which is usually done in interactive mode.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Since no cleaning/exploring was needed, my codes seem to be run in batch, not interactively. This is particularly refreshing to me, because I’ve been trying to do more in batch mode at work nowadays, and seems like I was already doing that even back then! But again, that’s because there was no need for heavy use of interactive exploration for simulated data. Also I’m sure I did quite extensive typing in interactive mode back then just to make sure my commands work.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;There was one main script that consists of many lines of codes, in which I used multiple for loops for various combinations of “parameters”. Now that I know a little bit about functional programming and bash programming, I can’t help but wonder how much the codes could have been rewritten (refactored).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I defined several functions, which are called in the main script&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. What was interesting though was that each function was defined and stored in its own file! At first I wondered why I had saved a one liner function in its own file, but seems like that was how it needed to be done back then.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I had one README type file that explains what each file is in that particular “program” (directory). I’m actually quite impressed by this file, because even though the file contains less than 20 lines of texts, it showed I tried to document something for potential users of the codes (probably was for future me!).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For each function, on the otherhand, I didn’t do a good job of describing what each input is supposed to be and do, let alone output. This is somewhat different for me nowadays, especially when I’m working on an R package. Documenting is actually quite enjoyable there, but the problem still occurs when I’m coding for data analysis. There is this tension between keeping scripts vs. writing codes and how much comments to provide, etc.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In some cases, there were hundreds of code files that seemingly differ only by certain part of their file names, indicating I could have benefited from version control and/or better parameterized/modular codes. Again, this seems to be an on-going struggle as to how to write better parameterized/modular codes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I didn’t really have a detailed directory structure. Pretty much everything was under one main “project” directory, and it was hard to distinguish which files were for which setting under the project. This is still relevant, because sometimes I find it hard to decide where to put each “project”: on its own, or under some existing projects?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It was quite a pleasant surprise running into my past code prouducts. I don’t think I’ll ever code in Matlab again, but it was still quite refreshing to learn how I used to do it, especially from the current point of view. I wonder how I’d feel about my current coding in say next 5-10 years!?&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Fortunately, I’ve kept those files in a portable device, which I’m not usually good at doing.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Interestingly, it seems I didn’t have to “source” the files to use the functions. Matlab must have known where to look for the function names.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;E.g., see &lt;a href=&#34;http://www.mathworks.com/help/matlab/matlab_prog/create-functions-in-files.html&#34;&gt;here.&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

        
      </description>
    </item>
    
    <item>
      <title>Comments on data analysis workflow</title>
      <link>/post/2018/07/22/comments-on-data-analysis-workflow/</link>
      <pubDate>Sun, 22 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/07/22/comments-on-data-analysis-workflow/</guid>
      <description>
        &lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;There are several benefits of establishing a good and routine data analysis workflow that you follow on a daily basis. At least two benefits come to mind immediately.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Having a good data analysis workflow is beneficial and needed to do reproducible research/work (RR). RR could mean one thing to one and quite another to others, but to me, doing reproducible work means specifically doing my work in a fashion that allows me to pick up from last touched point in that particular work after some break. For example, after spending 2-3 days on project A, I might go off to do something else for the next 2-3 days, and when I come back to project A, I want to know exactly what’s been done and what I need to do. There are many components of data analysis workflow that can be helpful for one to do RR, and I try to outline them below.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A routine workflow helps us avoid mental fatigue. This may be from one of the books by Hadley, but to me, this means knowing what, where, why, and how to do things right off the bat when starting a new project. Slowly but surely, I’m settling down to a specific approach that I’ll touch upon later, but to get to this point, I tried several different alternatives, and many times ended up failing, for example, to keep track of how to do things (e.g., refresh data with new source data) all over again when needed. Typical challenges have been, but are not limited to, (1) keeping track of data generation, preparation, and movement steps, (2) having to switch between several environments to do specific tasks (e.g., visual inspection), and (3) saving analysis summary and results in a place that’s as close to the scripts as possible.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;history&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;History&lt;/h2&gt;
&lt;p&gt;Next, I outline what I have tried so far with a quick rundown of pros and cons for each, beginning with the earliest approach I tried at my current work.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Local RStudio: suitable for most of my personal needs, I’ve been using RStudio on my local computer for a while before joining current work. And even at current work, RStudio was my first tool of choice.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pros
&lt;ul&gt;
&lt;li&gt;Quick and easy start: fire up an RStudio session (preferably at a Rproj level) and you’re ready for work.&lt;/li&gt;
&lt;li&gt;One stop shop: everything you need, you can do it in RStudio pretty much, such as visuals, Rmarkdown reports, blogging, version control (git), etc.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Cons
&lt;ul&gt;
&lt;li&gt;Privacy and security: the biggest drawback of using RStudio at work is that many times we’re not allowed to download the data to local computer due to privacy and security reasons. This is one single biggest roadblock that ultimately kept me from using it as a tool of choice at work.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Remote RCloud: an in-house analytics and visualization platform, it allows visual inspections, collaboration among data folks, and many other things, such as shiny app. Once RStudio turned out to be no-go for most tasks at work, I turned to RCloud for everything from data loading and summary, back when it was relatively early (yet still stable) in the development stage.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pros
&lt;ul&gt;
&lt;li&gt;Work with company data (small and big): since it’s built in-house, it’s capable of consuming company data.&lt;/li&gt;
&lt;li&gt;Visualization: unlike a remote R shell, visuals in RCloud is just like that in RStudio.&lt;/li&gt;
&lt;li&gt;Cells with not just R, but shell, python, shiny, etc.: RCloud can be used to do work not just in R, but also in python and shell.&lt;/li&gt;
&lt;li&gt;Notebooks: data generation, preparation, plots, and reports can be all saved in a sensible manner.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Cons
&lt;ul&gt;
&lt;li&gt;Connection sometimes lost (usually for long running jobs): mostly in its early days, I experienced RCloud hanging from time to time especially for long running jobs, typically big data ingestion, complex tasks (modeling), etc.&lt;/li&gt;
&lt;li&gt;Need internet/browser: not a biggie, but sometimes need to log in too, and it’s not as quick and easy as starting up a local RStudio session, for example.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Remote R shell: log on to work server, and start up R session!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pros
&lt;ul&gt;
&lt;li&gt;Quick and easy start: just like starting up a local RStudio, starting up a remote R shell is quick and easy, once sshed into remote servers.&lt;/li&gt;
&lt;li&gt;Can work with company data (usually small, but sometimes big too): it’s safe to work with company data in the remote servers, and depending on the need, rather big data can be explored in a remote R session too.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Cons
&lt;ul&gt;
&lt;li&gt;Can’t do visual inspections: one biggest hurdle with this option is its lack of graphic support.&lt;/li&gt;
&lt;li&gt;Many times data come from various sources and need to be prepared in a separate environment (e.g., hive)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pyspark in remote python shell&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pros
&lt;ul&gt;
&lt;li&gt;Scalable solutions for production deliverables&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Cons
&lt;ul&gt;
&lt;li&gt;Spark environment in general fails with error messages that are not easy to back track (i.e., I don’t know what I’m doing wrong)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sparklyr in remote R shell&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pros
&lt;ul&gt;
&lt;li&gt;Scalable solutions in familiar R environment&lt;/li&gt;
&lt;li&gt;Familiar environment (basically R shell)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Cons
&lt;ul&gt;
&lt;li&gt;Unstable? Connection lost frequently (probably because I’m doing something wrong)&lt;/li&gt;
&lt;li&gt;No visualization (same problem as in remote R shell)&lt;/li&gt;
&lt;li&gt;Set up time is “long” compared to non spark R shell&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sparklyr in RCloud&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pros
&lt;ul&gt;
&lt;li&gt;Scalable solutions in familiar R environment&lt;/li&gt;
&lt;li&gt;Visualization&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Cons
&lt;ul&gt;
&lt;li&gt;Big data visualization is not too common (e.g., many times need aggregation/summarization for plotting anyway, which doesn’t need to happen in spark, but in hive)&lt;/li&gt;
&lt;li&gt;Internet/browser&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;current&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Current&lt;/h2&gt;
&lt;p&gt;Before starting to settle down on a particular workflow, I realized that first thing I needed to do was to identify my day-to-day needs. Not exhaustively, answers to below questions may provide better insights to what we do on a daily basis.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What kind of data do I work with the most?&lt;/li&gt;
&lt;li&gt;Where do the data come from?&lt;/li&gt;
&lt;li&gt;What do I do with the data?&lt;/li&gt;
&lt;li&gt;Do I need interactive environment or batch jobs?&lt;/li&gt;
&lt;li&gt;How about ML? Is production ML an important part of my job?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It turns out that more often than not, majority of my day-to-day needs are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explore data stored in hive tables&lt;/li&gt;
&lt;li&gt;Visualize data stored in hive tables&lt;/li&gt;
&lt;li&gt;Summarize/document analysis results&lt;/li&gt;
&lt;li&gt;Train ML models and deliver scores in production&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So it became apparent that the integration of R and hive is rather important in my day-to-day work, and that I still rely much on an interactive environment especially during early stages of a project (aka exploration), which takes indeed the majority of the project time.&lt;/p&gt;
&lt;p&gt;Hence, I needed to come up with a better way to use data stored in hive tables in an interactive R session. After several trials-and-errors, I came to the conclusion that the combination of bash shell scripts, remote R shell, and RCloud should do for majority of what I do during this stage. This workflow uses bash shell scripts and remote R shell for quick data exploration and additional data prep, typically followed by RCloud for visualization and summary/documentation.&lt;/p&gt;
&lt;p&gt;Below, I describe how this workflow typically works.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Create a project directory and start up remote R shell from there&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One screen session and one R shell per project (hence don’t be afraid to have multiple screen sessions running)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Quick/iterative exploration in remote R shell&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data generation and preparation
&lt;ul&gt;
&lt;li&gt;Typically involves (intermediate) hive table generations and loading them in R (hive sql, bash, R shell)&lt;/li&gt;
&lt;li&gt;Use custom bash scripts and internal R packages for common tasks&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Descriptive
&lt;ul&gt;
&lt;li&gt;Typical R operations without visuals&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Additional data prep needed for further actions
&lt;ul&gt;
&lt;li&gt;Typically for visual inspection in RCloud&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Intermediate data saving
&lt;ul&gt;
&lt;li&gt;Typically in RDS format for visual inspections in RCloud&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Iterate above steps as needed&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Visual inspection and summary in RCloud&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a new notebook in RCloud (under a proper higher level directory)&lt;/li&gt;
&lt;li&gt;Start documenting findings from the exploration step&lt;/li&gt;
&lt;li&gt;Visual inspection
&lt;ul&gt;
&lt;li&gt;Using RDS files saved from quick explore step&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Reports and summary
&lt;ul&gt;
&lt;li&gt;Quick notes on findings according to the visuals&lt;/li&gt;
&lt;li&gt;Also quick notes on further actions, if needed&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Further actions typically involve ML&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Remote R shell for POC&lt;/li&gt;
&lt;li&gt;Remote R/python programming&lt;/li&gt;
&lt;li&gt;Scalable solution if needed (Spark)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This workflow is not free of pitfalls of course. Some pros and cons are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Pros&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It allows a quick project start-up and exploration.&lt;/li&gt;
&lt;li&gt;Data generation, prep, movement, and analysis steps are all stored in once file (explore.R).&lt;/li&gt;
&lt;li&gt;Visual inspection and progress/insights summary are all in one place (RCloud).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Cons&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It involves many data writes, which can be costly in terms of time and disk space.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;links&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tidyverse.org/articles/2017/12/workflow-vs-script/&#34; class=&#34;uri&#34;&gt;https://www.tidyverse.org/articles/2017/12/workflow-vs-script/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://r4ds.had.co.nz/explore-intro.html&#34; class=&#34;uri&#34;&gt;http://r4ds.had.co.nz/explore-intro.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://edwinth.github.io/blog/workflow/&#34; class=&#34;uri&#34;&gt;https://edwinth.github.io/blog/workflow/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cacm.acm.org/blogs/blog-cacm/169199-data-science-workflow-overview-and-challenges/fulltext&#34; class=&#34;uri&#34;&gt;https://cacm.acm.org/blogs/blog-cacm/169199-data-science-workflow-overview-and-challenges/fulltext&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

        
      </description>
    </item>
    
    <item>
      <title>External presentation goal</title>
      <link>/post/2018/07/22/external-presentation-goal/</link>
      <pubDate>Sun, 22 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/07/22/external-presentation-goal/</guid>
      <description>
        &lt;p&gt;Like Yihui (of blogdown and many other awesome R packages) whose goal is to publish a book a year&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, I have a similar personal goal that I started last year. While I’d love to write a book a year, it’s too ambitious a goal for me (and many people in general, I’d think) to pursue. Instead, my personal goal is to do an external presentation a year, be it for meetups, conferences, or nearby schools as a guest speaker.&lt;/p&gt;
&lt;p&gt;Back in May, I did a presentation on R package development for the ATL R User group, successfully completing the mission for the year :) The presentation has two decks: (1) main deck (&lt;a href=&#34;https://joongsup.rbind.io/slides/r_pkg_devel_final.pdf&#34;&gt;here&lt;/a&gt;) containing the introduction and high level overview, and (2) reference deck&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; (&lt;a href=&#34;https://joongsup.rbind.io/slides/r_pkg_devel.html&#34;&gt;here&lt;/a&gt;) containing the screenshots of each step of the R package development that I covered during the presentation.&lt;/p&gt;
&lt;p&gt;I should probably add to my personal goal list something about blog posting&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. I’m just happy still that blogging is this easy (again thanks to Yihui!), and so there should be no excuse why I can’t write a post more frequently. Well, I won’t make it an official personal goal just yet, but for now, I will have to keep writing posts whenever I can, sometimes multiple posts at one sitting, just like today! :)&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Amazingly, he’s successfully written a book a year for the last couple of years as far as I know.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;which I talked about &lt;a href=&#34;https://joongsup.rbind.io/post/2018/05/17/insert-images-in-blogdown-post/&#34;&gt;here&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Yihui’s frequent blog posting is also amazing.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

        
      </description>
    </item>
    
    <item>
      <title>Insert images in blogdown post, static directory, and xaringan</title>
      <link>/post/2018/05/17/insert-images-in-blogdown-post/</link>
      <pubDate>Thu, 17 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/05/17/insert-images-in-blogdown-post/</guid>
      <description>
        &lt;p&gt;While working on yet another separate blog post, I needed to insert images in the post. I knew from rmarkdown syntax that I can use: ![image name](path to file), but then I didn’t know where the image files need to be.&lt;/p&gt;
&lt;p&gt;It turns out I can have the image files under the &lt;a href=&#34;https://bookdown.org/yihui/blogdown/static-files.html&#34;&gt;static/&lt;/a&gt; directory, everything under which will be copied to public directory.&lt;/p&gt;
&lt;p&gt;Moreover, static/ directory can also be used to build Rmd documents (e.g., pdf and HTML5), and I decided to try generating an HTML5 slide using &lt;a href=&#34;https://slides.yihui.name/xaringan/#1&#34;&gt;xaringan&lt;/a&gt; package. It took some trial-and-error but eventually I got it working and here’s the &lt;a href=&#34;https://joongsup.rbind.io/slides/r_pkg_devel.html&#34;&gt;slide&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;(Next, maybe I should consider breaking out posts and slides.)&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>R package development walk-thru</title>
      <link>/post/2018/05/11/r-package-development-walk-thru/</link>
      <pubDate>Fri, 11 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/05/11/r-package-development-walk-thru/</guid>
      <description>
        &lt;div id=&#34;create-a-new-rstudio-project.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Create a new RStudio project.&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/img/new_project.png&#34; alt=&#34;project&#34; /&gt;
&lt;img src=&#34;/img/new_directory.png&#34; alt=&#34;directory&#34; /&gt;
&lt;img src=&#34;/img/new_r_pkg.png&#34; alt=&#34;pkg&#34; /&gt;
Next the best part so far: we have to give the new package a name!
&lt;img src=&#34;/img/new_pkg_name.png&#34; alt=&#34;name&#34; /&gt;
&lt;img src=&#34;/img/new_pkg_name_ready.png&#34; alt=&#34;name ready&#34; /&gt;
Once new project is created, this is the default contents.
&lt;img src=&#34;/img/hello_world.png&#34; alt=&#34;hello world&#34; /&gt;
We’ll go to project options menu and check roxygen option for documentation.
&lt;img src=&#34;/img/project_option.png&#34; alt=&#34;project option&#34; /&gt;
&lt;img src=&#34;/img/roxygen.png&#34; alt=&#34;roxygen&#34; /&gt;
&lt;img src=&#34;/img/roxygen_options.png&#34; alt=&#34;roxygen options&#34; /&gt;
&lt;img src=&#34;/img/roxygen_ready.png&#34; alt=&#34;roxygen ready&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now let’s create our own function.
&lt;img src=&#34;/img/new_r_script.png&#34; alt=&#34;r script&#34; /&gt;
&lt;img src=&#34;/img/r_script_name.png&#34; alt=&#34;r script name&#34; /&gt;
&lt;img src=&#34;/img/load_all.png&#34; alt=&#34;load all&#34; /&gt;
Let’s test the new function by loading the functions. No need for sourcing anymore!
&lt;img src=&#34;/img/load_all_done.png&#34; alt=&#34;loaded&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now that the function is working as intended, let’s document the function for others, including future us.
&lt;img src=&#34;/img/insert_doc.png&#34; alt=&#34;insert roxygen&#34; /&gt;
&lt;img src=&#34;/img/doc_done.png&#34; alt=&#34;document done&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Keyboard shortcut: cmd-shift-D generates the documents.
&lt;img src=&#34;/img/generate_doc.png&#34; alt=&#34;generate document&#34; /&gt;
&lt;img src=&#34;/img/check_doc.png&#34; alt=&#34;check document&#34; /&gt;
Let’s remove the default contents.
&lt;img src=&#34;/img/rm_hello.png&#34; alt=&#34;delete hello&#34; /&gt;
Now that we have working functions and associated documents, let’s finish up writing functions/docs, and install the R package we just wrote.&lt;/p&gt;
&lt;p&gt;Keyboard shortcut: cmd-shift-B
&lt;img src=&#34;/img/install.png&#34; alt=&#34;install and restart&#34; /&gt;
Let’s check if the package is without any issues.&lt;/p&gt;
&lt;p&gt;Keyboard shortcut: cmd-shift-E
&lt;img src=&#34;/img/check_pkg.png&#34; alt=&#34;check package&#34; /&gt;
&lt;img src=&#34;/img/check_progress.png&#34; alt=&#34;check progress&#34; /&gt;
Check result is not clean. There is a warning and a note. We’ll need to address them.
&lt;img src=&#34;/img/check_warning.png&#34; alt=&#34;check warning&#34; /&gt;
Let’s fix importFrom.
&lt;img src=&#34;/img/check_warning_fixed1.png&#34; alt=&#34;check import fixed&#34; /&gt;
Then let’s fix DESCRIPTION.
&lt;img src=&#34;/img/check_warning_fixed2.png&#34; alt=&#34;check description fixed&#34; /&gt;
Once all issues are addressed, we see the check result is clean.&lt;/p&gt;
&lt;p&gt;Let’s build the source package (no shortcut)
&lt;img src=&#34;/img/build_src.png&#34; alt=&#34;build source package&#34; /&gt;
&lt;img src=&#34;/img/build_src_progress.png&#34; alt=&#34;build source progress&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;share&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Share&lt;/h2&gt;
&lt;p&gt;Three different ways to share our new R package.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;If you are the only user of the R package, then no additional action is required. cmd-shift-B (install step) installed the development version of the package already.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you want to share your R package with co-workers, you can (sftp) copy the zipped/built source file to a common location and have you co-workers them install the package in their R session by install.packages(“/path/to/pkg”, repos = NULL)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/build_src_progress.png&#34; alt=&#34;Setup git&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Setup git&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;/img/restart_after_git.png&#34; alt=&#34;Restart after git&#34; /&gt;
&lt;img src=&#34;/img/git_pane.png&#34; alt=&#34;Git pane&#34; /&gt;
&lt;img src=&#34;/img/commit.png&#34; alt=&#34;commit&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;If you want to share your R package with total strangers out there, you can share all source files in github (or any other version control systems) and have them install the package in their R session by devtools::install_github(“user/repo”)&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

        
      </description>
    </item>
    
    <item>
      <title>Random Forest Revisited</title>
      <link>/post/2018/05/11/random-forest-revisited/</link>
      <pubDate>Fri, 11 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/05/11/random-forest-revisited/</guid>
      <description>
        &lt;div id=&#34;hypothetical-setting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hypothetical setting&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Objective: binary classification&lt;/li&gt;
&lt;li&gt;N = 5M observations&lt;/li&gt;
&lt;li&gt;p = 10 variables (5 categorical and 5 continuous variables)&lt;/li&gt;
&lt;li&gt;ntree = 100 (model will not be adversely affected if ntree is too big)&lt;/li&gt;
&lt;li&gt;mtry = number of predictors to consider at each split (fixed or determined by resampling (10-fold cv)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;bootstrap-samples-for-each-tree&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bootstrap samples for each tree&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Bootstrap samples (sampling with replacement) of same size as the original data (N) is taken at each tree.&lt;/li&gt;
&lt;li&gt;This results in about 1/3 of N (hence approx. 1.6M) samples never being chosen for each tree. This is called out of bag (OOB) samples.&lt;/li&gt;
&lt;li&gt;This OOB samples are used for:
&lt;ul&gt;
&lt;li&gt;Unbiased estimate of test set error
&lt;ul&gt;
&lt;li&gt;Theoretically, there’s no need for cv or a separate test set to get an unbiased estimate of the test set error, as it’s estimated internally during tree constructions.&lt;/li&gt;
&lt;li&gt;OOB error estimate is the proportion of times (over all x_is in overall OOB samples) that predicted class is different from the true class for sample x&lt;sub&gt;i&lt;/sub&gt;. Note the prediction for x&lt;sub&gt;i&lt;/sub&gt; is obtained using only the trees that did not have x&lt;sub&gt;i&lt;/sub&gt; in bootstrap sample.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Estimate of variable importance: see variablem importance section below.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;split-criteria&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Split criteria&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;At each split, the mtry number of predictors are randomly selected (to de-correlate the trees), and a splitting variable is obtained.
&lt;ul&gt;
&lt;li&gt;The spliting variable is the one that results in the most homogeneous descendents (nodes).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Once the best mtry is fixed (or obtained via resampling, such as cv), caret (randomForest too?) fits the final model using the best mtry and save to finalModel.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;categorical-variable-treatment-various-encoding-methods&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Categorical variable treatment (various encoding methods)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Has implications in variable importance!
&lt;ul&gt;
&lt;li&gt;How to “aggregate” one-hot encoded feature importance!?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Has implications in grid search step!
&lt;ul&gt;
&lt;li&gt;Grid search for best mtry (in k-fold cv) might result in different result for mtry&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Various encoding methods
&lt;ul&gt;
&lt;li&gt;Label encoding (integer encoding/string indexing)
&lt;ul&gt;
&lt;li&gt;Simple hashing/lookup (dog –&amp;gt; 0, cat –&amp;gt; 1, etc.)&lt;/li&gt;
&lt;li&gt;If not the categorical variable is not ordinal, arbitrary indexing can be problematic (e.g., why dog (=0) &amp;lt; cat (=1)).
&lt;ul&gt;
&lt;li&gt;Python/Pyspark solves this issue by providing an ordering structure (i.e., 0 to the most frequent item, 1 to the second frequent, and so on).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;One-hot encoding (dummy variable)
&lt;ul&gt;
&lt;li&gt;Default method in caret (seems for both character and factor variables)
&lt;ul&gt;
&lt;li&gt;Default method in R maybe?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Is one-hot encoding bad? See [&lt;a href=&#34;http://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/&#34; class=&#34;uri&#34;&gt;http://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Binary encoding
&lt;ul&gt;
&lt;li&gt;Seems interesting, but see this:&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;continuous-variable-treatment-binning-vs.raw&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Continuous variable treatment (binning vs. raw)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;For customers subscription based analyses, it seems customary to bin some continuous variables such as tenures into bins, instead of using raw numbers.
&lt;ul&gt;
&lt;li&gt;Does this make your model rather robust?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;variable-importance-interpretations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Variable importance interpretations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;From the original randomForest package (and not from caret)
&lt;ul&gt;
&lt;li&gt;Mean decrease in accuracy (by permutation test, type = 1)
&lt;ul&gt;
&lt;li&gt;How’s permutation done?&lt;/li&gt;
&lt;li&gt;Randomly permuting the values of each predictor for the OOB sample of one predictor at a time for each tree.&lt;/li&gt;
&lt;li&gt;The difference in predictive performance between the non-permted sample and the permuted sample for each preditor is recorded and aggregated acoross the entire forest.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Mean decrease in impurity (no need for additional test, type = 2): total decrease in impurities resulting from using variable k as a splitter, averaged over all trees.
&lt;ul&gt;
&lt;li&gt;“… is often very consistent with the permutation importance measure”, from [&lt;a href=&#34;https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#varimp&#34; class=&#34;uri&#34;&gt;https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#varimp&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;What am I getting from caret::varImp vs. randomForest::importance?
&lt;ul&gt;
&lt;li&gt;By default, it seems both caret::varImp and randomForest::importance gives the impurity measure (type = 2)&lt;/li&gt;
&lt;li&gt;randomForest::importance(caret_model_object&lt;span class=&#34;math inline&#34;&gt;\(fit\)&lt;/span&gt;finalModel) gives the MeanDescreaseGini importance values, and so does caret::varImp(caret_model_object$fit, scale = FALSE)!!&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Variable selection/importance quality?
&lt;ul&gt;
&lt;li&gt;Relevant to different categorical encoding method (e.g., one-hot encoding results in individual level vs. label encoding just one)&lt;/li&gt;
&lt;li&gt;See Strobl et al. [&lt;a href=&#34;https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-25&#34; class=&#34;uri&#34;&gt;https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-25&lt;/a&gt;]
&lt;ul&gt;
&lt;li&gt;RF favors continuous variables and categoricals with many levels
&lt;ul&gt;
&lt;li&gt;So if all variables are categoricals with relatively small number of levels, prob ok for now&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;class-imbalance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Class imbalance&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;See &lt;a href=&#34;https://dpmartin42.github.io/posts/r/imbalanced-classes-part-1&#34; class=&#34;uri&#34;&gt;https://dpmartin42.github.io/posts/r/imbalanced-classes-part-1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

        
      </description>
    </item>
    
    <item>
      <title>Vim mode in RStudio</title>
      <link>/post/2018/05/11/vim-mode-in-rstudio/</link>
      <pubDate>Fri, 11 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/05/11/vim-mode-in-rstudio/</guid>
      <description>
        &lt;p&gt;I heard about Vim mode in RStudio but haven’t really given it a try. While working on a new blog post (not this one), I decided to give it a try, as I had to keep switching between non-Vim mode on my local Mac and Vim mode in my remote server (Linux). I’m not a Vim expert by any measure, but for some reason, I like working in Vim.&lt;/p&gt;
&lt;p&gt;So the option is in Tools/Global Options/Code/Key Bindings, and boom, I started using Vim in RStudio. One thing I ran into immediately, though, was that the cursor movement keys (h/j/k/l) wouldn’t repeat themselves when I kept them pressed. At first I thought maybe it’s one of those things that I’d have to live with, but quickly it became rather uncomfortable having to repeatedly pressing j/k keys to go up and down.&lt;/p&gt;
&lt;p&gt;Turns out all I had to do was to change system reference (on my Mac) as explained in this &lt;a href=&#34;https://support.rstudio.com/hc/en-us/articles/204896737&#34;&gt;support page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I feel like there must be some more (unexpected) additional features that come with Vim mode in RStudio (not sure if it’s intended or not), such as file save by cmd-s, which I find really helpful, because I don’t have to do esc-:-w just to save file :)&lt;/p&gt;

        
      </description>
    </item>
    
  </channel>
</rss>
