<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jay&#39;s Notes</title>
    <link>/</link>
    <description>Recent content on Jay&#39;s Notes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 05 Aug 2018 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Home sweet dome</title>
      <link>/post/2018/08/05/home-sweet-dome/</link>
      <pubDate>Sun, 05 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/08/05/home-sweet-dome/</guid>
      <description>
        &lt;p&gt;I ran into a &lt;a href=&#34;https://www.tarheelblog.com/2018/8/4/17644436/unc-basketball-tar-heels-dean-dome-advantage&#34;&gt;post&lt;/a&gt; on the Tar Heel Blog (THB) that talks about the Tar Heel’s home court advantage in recent years. Since it’s been a while since I wrote anything about UNCMBB, I thought it’d be a great topic to write on here too, looking at how great the teams have played on home court&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; in recent years. And maybe I’ll look at how Duke has played on their home court too during the same time period just because.&lt;/p&gt;
&lt;p&gt;THB counted the wins and losses in the past 3 years in particular, and that gave me a starting point.&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Season
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Where
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
wins
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
losses
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2016
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
H
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
H
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
H
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;So it seems my count and their count matches. Then I was curious if this last 3 years have been the best there is, in terms of fewest losses, and for that matter, most wins, and best winning percentage at home. In below charts, season is the starting season of the 3 years. E.g., for season = 2016, value = 4 means, rolling 3 years loss count for 2016, 2017, and 2018 seasons. Championship seasons are represented with larger dots, and the latest 3 year statistics (wins, losses, and winning percentages) with coloured horizontal lines.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-05-home-sweet-dome_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Although this past 3 years loss count (4) is a great feat, it’s not the best there is, it turns out, and there’s been 18 instances with fewer than 4 loss count. Three early championship seasons (1957, 1982, and 1983) stand out with 3 or fewer losses each (again, as the starting season of the next 3 seasons), with the next two championship seasons (2005 and 2009) not so superb loss counts at 6 apiece&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The fewest 3-yr rolling home loss counts came in two separate instances, with a single lose apiece: 1977/1978/1979 seasons and 1978/1979/1980 seasons. Let’s see what those losses were.&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Season
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Game_Date
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Game_Day
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Type
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Where
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Opponent_School
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Result
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Tm
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Opp
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
OT
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1977
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1977-01-26
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Wed
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
REG
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
H
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Wake Forest
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
L
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
66
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
67
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1980
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1980-01-20
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Sun
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
REG
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
H
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Maryland
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
L
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
86
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
92
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Nothing stands out at me right away about Carolina’s 1977 - 1980 teams&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;, let alone 1977 Wake Forest and 1980 Maryland, but it must have been fantastic home court winning streaks for the Tar Heels. Below shows the 3-yr rolling win counts.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-05-home-sweet-dome_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Sure, 1977 to 1979 seasons show great home winning trends at the time (e.g., 35 home wins during 3 seasons starting from 1978 season was the most in UNC history&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; at the time), but the number of games played are somewhat different year after year, so let’s look at the winning percentage instead.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-05-home-sweet-dome_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Yet again, 3 years following 1977 and 1978 seasons showed the highest rolling 3-yr home winning percentages, so it must have been really fun watching Carolina playing and winning at home those days. I feel like I have some homework to do, getting to know them a little better.&lt;/p&gt;
&lt;p&gt;Now that I’ve looked at a couple of statistics for rolling 3 years, I became curious which freshman class has the bragging rights in terms of fewest losses, most wins, and highest winning percentage over their college carrers at home court. This should be pretty straightforward since a typical college career lasts for 4 years, which means all I have to change is the number of rolling years from 3 to 4. Below charts show the rolling 4-yr losses, wins, and win percentages, respectively.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-05-home-sweet-dome_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-05-home-sweet-dome_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-05-home-sweet-dome_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For Carolina, it seems the bragging rights go to the freshman classes in the late 70s whereas for Duke, they could go to any of the teams in the late 80s, early and late 2000s. I had expected Psycho-T’s 2005 freshman class to stand out in any of the three numbers, but to my surprise the class wasn’t the top although they were close especially in terms of rolling 4-yr wins. That got me thinking, maybe they did much better in away games (after all, that class never lost to Duke on their home court during their college careers), and that’s what I’ll be looking at next time.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Which, by the way, is deservingly going to be named after Coach Roy Williams.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Most recent championship season (2017) does not have 3-yr rolling counts yet.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Just a casual fan here :)&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;Since 1949-1950 season to be exact.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

        
      </description>
    </item>
    
    <item>
      <title>My old coding products</title>
      <link>/post/2018/07/27/my-old-coding-products/</link>
      <pubDate>Fri, 27 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/07/27/my-old-coding-products/</guid>
      <description>
        &lt;p&gt;Lately I’ve been thinking about why I care much about everything R and sharing the joy of using R, which deserves its own post. Much of it has to do with how I did and did not get a proper training in coding suitable for data analysis in the past, but as I looked back on my personal coding history, I came across hundreds of code files that I wrote in the past&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. Suffice to say, they were pretty important files in my education history, but I have totally forgotten about them until now.&lt;/p&gt;
&lt;p&gt;They are .m files, meant to be used in Matlab. As I started opening and reading a couple of those files, I had mixed feelings, ranging from agony (“omg, this code was from that stressful point in time”) and embarrassment (“omg, look at the logic and style I was using there”), to delight (“wow I’m defining several functions to be used in main script!”) and motivation (“Yup, I was this bad, and that’s exactly why I care about education in coding (especially R) for data analysis!”).&lt;/p&gt;
&lt;p&gt;It was quite refreshing to see how I used to code in Matlab, which had some similarities with R, hence giving me some perspectives on how my data analysis coding skills might have changed. Here are some observations of the past, in conjunction with present whenever applicable:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Since the purpose of the code files (“program”) was to show my particular engineering method works as intended, the codes were relatively simple having simluated data as its input data. There was no cleaning/exploring step, whereas now, where almost always, there’s no clean data, thus always having to write cleaning/exploring scripts, which is usually done in interactive mode.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Since no cleaning/exploring was needed, my codes seem to be run in batch, not interactively. This is particularly refreshing to me, because I’ve been trying to do more in batch mode at work nowadays, and seems like I was already doing that even back then! But again, that’s because there was no need for heavy use of interactive exploration for simulated data. Also I’m sure I did quite extensive typing in interactive mode back then just to make sure my commands work.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;There was one main script that consists of many lines of codes, in which I used multiple for loops for various combinations of “parameters”. Now that I know a little bit about functional programming and bash programming, I can’t help but wonder how much the codes could have been rewritten (refactored).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I defined several functions, which are called in the main script&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. What was interesting though was that each function was defined and stored in its own file! At first I wondered why I had saved a one liner function in its own file, but seems like that was how it needed to be done back then.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I had one README type file that explains what each file is in that particular “program” (directory). I’m actually quite impressed by this file, because even though the file contains less than 20 lines of texts, it showed I tried to document something for potential users of the codes (probably was for future me!).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For each function, on the otherhand, I didn’t do a good job of describing what each input is supposed to be and do, let alone output. This is somewhat different for me nowadays, especially when I’m working on an R package. Documenting is actually quite enjoyable there, but the problem still occurs when I’m coding for data analysis. There is this tension between keeping scripts vs. writing codes and how much comments to provide, etc.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In some cases, there were hundreds of code files that seemingly differ only by certain part of their file names, indicating I could have benefited from version control and/or better parameterized/modular codes. Again, this seems to be an on-going struggle as to how to write better parameterized/modular codes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I didn’t really have a detailed directory structure. Pretty much everything was under one main “project” directory, and it was hard to distinguish which files were for which setting under the project. This is still relevant, because sometimes I find it hard to decide where to put each “project”: on its own, or under some existing projects?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It was quite a pleasant surprise running into my past code prouducts. I don’t think I’ll ever code in Matlab again, but it was still quite refreshing to learn how I used to do it, especially from the current point of view. I wonder how I’d feel about my current coding in say next 5-10 years!?&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Fortunately, I’ve kept those files in a portable device, which I’m not usually good at doing.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Interestingly, it seems I didn’t have to “source” the files to use the functions. Matlab must have known where to look for the function names.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;E.g., see &lt;a href=&#34;http://www.mathworks.com/help/matlab/matlab_prog/create-functions-in-files.html&#34;&gt;here.&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

        
      </description>
    </item>
    
    <item>
      <title>Comments on data analysis workflow</title>
      <link>/post/2018/07/22/comments-on-data-analysis-workflow/</link>
      <pubDate>Sun, 22 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/07/22/comments-on-data-analysis-workflow/</guid>
      <description>
        &lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;There are several benefits of establishing a good and routine data analysis workflow that you follow on a daily basis. At least two benefits come to mind immediately.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Having a good data analysis workflow is beneficial and needed to do reproducible research/work (RR). RR could mean one thing to one and quite another to others, but to me, doing reproducible work means specifically doing my work in a fashion that allows me to pick up from last touched point in that particular work after some break. For example, after spending 2-3 days on project A, I might go off to do something else for the next 2-3 days, and when I come back to project A, I want to know exactly what’s been done and what I need to do. There are many components of data analysis workflow that can be helpful for one to do RR, and I try to outline them below.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A routine workflow helps us avoid mental fatigue. This may be from one of the books by Hadley, but to me, this means knowing what, where, why, and how to do things right off the bat when starting a new project. Slowly but surely, I’m settling down to a specific approach that I’ll touch upon later, but to get to this point, I tried several different alternatives, and many times ended up failing, for example, to keep track of how to do things (e.g., refresh data with new source data) all over again when needed. Typical challenges have been, but are not limited to, (1) keeping track of data generation, preparation, and movement steps, (2) having to switch between several environments to do specific tasks (e.g., visual inspection), and (3) saving analysis summary and results in a place that’s as close to the scripts as possible.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;history&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;History&lt;/h2&gt;
&lt;p&gt;Next, I outline what I have tried so far with a quick rundown of pros and cons for each, beginning with the earliest approach I tried at my current work.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Local RStudio: suitable for most of my personal needs, I’ve been using RStudio on my local computer for a while before joining current work. And even at current work, RStudio was my first tool of choice.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pros
&lt;ul&gt;
&lt;li&gt;Quick and easy start: fire up an RStudio session (preferably at a Rproj level) and you’re ready for work.&lt;/li&gt;
&lt;li&gt;One stop shop: everything you need, you can do it in RStudio pretty much, such as visuals, Rmarkdown reports, blogging, version control (git), etc.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Cons
&lt;ul&gt;
&lt;li&gt;Privacy and security: the biggest drawback of using RStudio at work is that many times we’re not allowed to download the data to local computer due to privacy and security reasons. This is one single biggest roadblock that ultimately kept me from using it as a tool of choice at work.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Remote RCloud: an in-house analytics and visualization platform, it allows visual inspections, collaboration among data folks, and many other things, such as shiny app. Once RStudio turned out to be no-go for most tasks at work, I turned to RCloud for everything from data loading and summary, back when it was relatively early (yet still stable) in the development stage.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pros
&lt;ul&gt;
&lt;li&gt;Work with company data (small and big): since it’s built in-house, it’s capable of consuming company data.&lt;/li&gt;
&lt;li&gt;Visualization: unlike a remote R shell, visuals in RCloud is just like that in RStudio.&lt;/li&gt;
&lt;li&gt;Cells with not just R, but shell, python, shiny, etc.: RCloud can be used to do work not just in R, but also in python and shell.&lt;/li&gt;
&lt;li&gt;Notebooks: data generation, preparation, plots, and reports can be all saved in a sensible manner.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Cons
&lt;ul&gt;
&lt;li&gt;Connection sometimes lost (usually for long running jobs): mostly in its early days, I experienced RCloud hanging from time to time especially for long running jobs, typically big data ingestion, complex tasks (modeling), etc.&lt;/li&gt;
&lt;li&gt;Need internet/browser: not a biggie, but sometimes need to log in too, and it’s not as quick and easy as starting up a local RStudio session, for example.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Remote R shell: log on to work server, and start up R session!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pros
&lt;ul&gt;
&lt;li&gt;Quick and easy start: just like starting up a local RStudio, starting up a remote R shell is quick and easy, once sshed into remote servers.&lt;/li&gt;
&lt;li&gt;Can work with company data (usually small, but sometimes big too): it’s safe to work with company data in the remote servers, and depending on the need, rather big data can be explored in a remote R session too.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Cons
&lt;ul&gt;
&lt;li&gt;Can’t do visual inspections: one biggest hurdle with this option is its lack of graphic support.&lt;/li&gt;
&lt;li&gt;Many times data come from various sources and need to be prepared in a separate environment (e.g., hive)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pyspark in remote python shell&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pros
&lt;ul&gt;
&lt;li&gt;Scalable solutions for production deliverables&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Cons
&lt;ul&gt;
&lt;li&gt;Spark environment in general fails with error messages that are not easy to back track (i.e., I don’t know what I’m doing wrong)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sparklyr in remote R shell&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pros
&lt;ul&gt;
&lt;li&gt;Scalable solutions in familiar R environment&lt;/li&gt;
&lt;li&gt;Familiar environment (basically R shell)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Cons
&lt;ul&gt;
&lt;li&gt;Unstable? Connection lost frequently (probably because I’m doing something wrong)&lt;/li&gt;
&lt;li&gt;No visualization (same problem as in remote R shell)&lt;/li&gt;
&lt;li&gt;Set up time is “long” compared to non spark R shell&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sparklyr in RCloud&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pros
&lt;ul&gt;
&lt;li&gt;Scalable solutions in familiar R environment&lt;/li&gt;
&lt;li&gt;Visualization&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Cons
&lt;ul&gt;
&lt;li&gt;Big data visualization is not too common (e.g., many times need aggregation/summarization for plotting anyway, which doesn’t need to happen in spark, but in hive)&lt;/li&gt;
&lt;li&gt;Internet/browser&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;current&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Current&lt;/h2&gt;
&lt;p&gt;Before starting to settle down on a particular workflow, I realized that first thing I needed to do was to identify my day-to-day needs. Not exhaustively, answers to below questions may provide better insights to what we do on a daily basis.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What kind of data do I work with the most?&lt;/li&gt;
&lt;li&gt;Where do the data come from?&lt;/li&gt;
&lt;li&gt;What do I do with the data?&lt;/li&gt;
&lt;li&gt;Do I need interactive environment or batch jobs?&lt;/li&gt;
&lt;li&gt;How about ML? Is production ML an important part of my job?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It turns out that more often than not, majority of my day-to-day needs are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explore data stored in hive tables&lt;/li&gt;
&lt;li&gt;Visualize data stored in hive tables&lt;/li&gt;
&lt;li&gt;Summarize/document analysis results&lt;/li&gt;
&lt;li&gt;Train ML models and deliver scores in production&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So it became apparent that the integration of R and hive is rather important in my day-to-day work, and that I still rely much on an interactive environment especially during early stages of a project (aka exploration), which takes indeed the majority of the project time.&lt;/p&gt;
&lt;p&gt;Hence, I needed to come up with a better way to use data stored in hive tables in an interactive R session. After several trials-and-errors, I came to the conclusion that the combination of bash shell scripts, remote R shell, and RCloud should do for majority of what I do during this stage. This workflow uses bash shell scripts and remote R shell for quick data exploration and additional data prep, typically followed by RCloud for visualization and summary/documentation.&lt;/p&gt;
&lt;p&gt;Below, I describe how this workflow typically works.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Create a project directory and start up remote R shell from there&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One screen session and one R shell per project (hence don’t be afraid to have multiple screen sessions running)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Quick/iterative exploration in remote R shell&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data generation and preparation
&lt;ul&gt;
&lt;li&gt;Typically involves (intermediate) hive table generations and loading them in R (hive sql, bash, R shell)&lt;/li&gt;
&lt;li&gt;Use custom bash scripts and internal R packages for common tasks&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Descriptive
&lt;ul&gt;
&lt;li&gt;Typical R operations without visuals&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Additional data prep needed for further actions
&lt;ul&gt;
&lt;li&gt;Typically for visual inspection in RCloud&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Intermediate data saving
&lt;ul&gt;
&lt;li&gt;Typically in RDS format for visual inspections in RCloud&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Iterate above steps as needed&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Visual inspection and summary in RCloud&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a new notebook in RCloud (under a proper higher level directory)&lt;/li&gt;
&lt;li&gt;Start documenting findings from the exploration step&lt;/li&gt;
&lt;li&gt;Visual inspection
&lt;ul&gt;
&lt;li&gt;Using RDS files saved from quick explore step&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Reports and summary
&lt;ul&gt;
&lt;li&gt;Quick notes on findings according to the visuals&lt;/li&gt;
&lt;li&gt;Also quick notes on further actions, if needed&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Further actions typically involve ML&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Remote R shell for POC&lt;/li&gt;
&lt;li&gt;Remote R/python programming&lt;/li&gt;
&lt;li&gt;Scalable solution if needed (Spark)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This workflow is not free of pitfalls of course. Some pros and cons are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Pros&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It allows a quick project start-up and exploration.&lt;/li&gt;
&lt;li&gt;Data generation, prep, movement, and analysis steps are all stored in once file (explore.R).&lt;/li&gt;
&lt;li&gt;Visual inspection and progress/insights summary are all in one place (RCloud).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Cons&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It involves many data writes, which can be costly in terms of time and disk space.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;links&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tidyverse.org/articles/2017/12/workflow-vs-script/&#34; class=&#34;uri&#34;&gt;https://www.tidyverse.org/articles/2017/12/workflow-vs-script/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://r4ds.had.co.nz/explore-intro.html&#34; class=&#34;uri&#34;&gt;http://r4ds.had.co.nz/explore-intro.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://edwinth.github.io/blog/workflow/&#34; class=&#34;uri&#34;&gt;https://edwinth.github.io/blog/workflow/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cacm.acm.org/blogs/blog-cacm/169199-data-science-workflow-overview-and-challenges/fulltext&#34; class=&#34;uri&#34;&gt;https://cacm.acm.org/blogs/blog-cacm/169199-data-science-workflow-overview-and-challenges/fulltext&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

        
      </description>
    </item>
    
    <item>
      <title>External presentation goal</title>
      <link>/post/2018/07/22/external-presentation-goal/</link>
      <pubDate>Sun, 22 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/07/22/external-presentation-goal/</guid>
      <description>
        &lt;p&gt;Like Yihui (of blogdown and many other awesome R packages) whose goal is to publish a book a year&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, I have a similar personal goal that I started last year. While I’d love to write a book a year, it’s too ambitious a goal for me (and many people in general, I’d think) to pursue. Instead, my personal goal is to do an external presentation a year, be it for meetups, conferences, or nearby schools as a guest speaker.&lt;/p&gt;
&lt;p&gt;Back in May, I did a presentation on R package development for the ATL R User group, successfully completing the mission for the year :) The presentation has two decks: (1) main deck (&lt;a href=&#34;https://joongsup.rbind.io/slides/r_pkg_devel_final.pdf&#34;&gt;here&lt;/a&gt;) containing the introduction and high level overview, and (2) reference deck&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; (&lt;a href=&#34;https://joongsup.rbind.io/slides/r_pkg_devel.html&#34;&gt;here&lt;/a&gt;) containing the screenshots of each step of the R package development that I covered during the presentation.&lt;/p&gt;
&lt;p&gt;I should probably add to my personal goal list something about blog posting&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. I’m just happy still that blogging is this easy (again thanks to Yihui!), and so there should be no excuse why I can’t write a post more frequently. Well, I won’t make it an official personal goal just yet, but for now, I will have to keep writing posts whenever I can, sometimes multiple posts at one sitting, just like today! :)&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Amazingly, he’s successfully written a book a year for the last couple of years as far as I know.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;which I talked about &lt;a href=&#34;https://joongsup.rbind.io/post/2018/05/17/insert-images-in-blogdown-post/&#34;&gt;here&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Yihui’s frequent blog posting is also amazing.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

        
      </description>
    </item>
    
    <item>
      <title>Insert images in blogdown post, static directory, and xaringan</title>
      <link>/post/2018/05/17/insert-images-in-blogdown-post/</link>
      <pubDate>Thu, 17 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/05/17/insert-images-in-blogdown-post/</guid>
      <description>
        &lt;p&gt;While working on yet another separate blog post, I needed to insert images in the post. I knew from rmarkdown syntax that I can use: ![image name](path to file), but then I didn’t know where the image files need to be.&lt;/p&gt;
&lt;p&gt;It turns out I can have the image files under the &lt;a href=&#34;https://bookdown.org/yihui/blogdown/static-files.html&#34;&gt;static/&lt;/a&gt; directory, everything under which will be copied to public directory.&lt;/p&gt;
&lt;p&gt;Moreover, static/ directory can also be used to build Rmd documents (e.g., pdf and HTML5), and I decided to try generating an HTML5 slide using &lt;a href=&#34;https://slides.yihui.name/xaringan/#1&#34;&gt;xaringan&lt;/a&gt; package. It took some trial-and-error but eventually I got it working and here’s the &lt;a href=&#34;https://joongsup.rbind.io/slides/r_pkg_devel.html&#34;&gt;slide&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;(Next, maybe I should consider breaking out posts and slides.)&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>R package development walk-thru</title>
      <link>/post/2018/05/11/r-package-development-walk-thru/</link>
      <pubDate>Fri, 11 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/05/11/r-package-development-walk-thru/</guid>
      <description>
        &lt;div id=&#34;create-a-new-rstudio-project.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Create a new RStudio project.&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/img/new_project.png&#34; alt=&#34;project&#34; /&gt;
&lt;img src=&#34;/img/new_directory.png&#34; alt=&#34;directory&#34; /&gt;
&lt;img src=&#34;/img/new_r_pkg.png&#34; alt=&#34;pkg&#34; /&gt;
Next the best part so far: we have to give the new package a name!
&lt;img src=&#34;/img/new_pkg_name.png&#34; alt=&#34;name&#34; /&gt;
&lt;img src=&#34;/img/new_pkg_name_ready.png&#34; alt=&#34;name ready&#34; /&gt;
Once new project is created, this is the default contents.
&lt;img src=&#34;/img/hello_world.png&#34; alt=&#34;hello world&#34; /&gt;
We’ll go to project options menu and check roxygen option for documentation.
&lt;img src=&#34;/img/project_option.png&#34; alt=&#34;project option&#34; /&gt;
&lt;img src=&#34;/img/roxygen.png&#34; alt=&#34;roxygen&#34; /&gt;
&lt;img src=&#34;/img/roxygen_options.png&#34; alt=&#34;roxygen options&#34; /&gt;
&lt;img src=&#34;/img/roxygen_ready.png&#34; alt=&#34;roxygen ready&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now let’s create our own function.
&lt;img src=&#34;/img/new_r_script.png&#34; alt=&#34;r script&#34; /&gt;
&lt;img src=&#34;/img/r_script_name.png&#34; alt=&#34;r script name&#34; /&gt;
&lt;img src=&#34;/img/load_all.png&#34; alt=&#34;load all&#34; /&gt;
Let’s test the new function by loading the functions. No need for sourcing anymore!
&lt;img src=&#34;/img/load_all_done.png&#34; alt=&#34;loaded&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now that the function is working as intended, let’s document the function for others, including future us.
&lt;img src=&#34;/img/insert_doc.png&#34; alt=&#34;insert roxygen&#34; /&gt;
&lt;img src=&#34;/img/doc_done.png&#34; alt=&#34;document done&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Keyboard shortcut: cmd-shift-D generates the documents.
&lt;img src=&#34;/img/generate_doc.png&#34; alt=&#34;generate document&#34; /&gt;
&lt;img src=&#34;/img/check_doc.png&#34; alt=&#34;check document&#34; /&gt;
Let’s remove the default contents.
&lt;img src=&#34;/img/rm_hello.png&#34; alt=&#34;delete hello&#34; /&gt;
Now that we have working functions and associated documents, let’s finish up writing functions/docs, and install the R package we just wrote.&lt;/p&gt;
&lt;p&gt;Keyboard shortcut: cmd-shift-B
&lt;img src=&#34;/img/install.png&#34; alt=&#34;install and restart&#34; /&gt;
Let’s check if the package is without any issues.&lt;/p&gt;
&lt;p&gt;Keyboard shortcut: cmd-shift-E
&lt;img src=&#34;/img/check_pkg.png&#34; alt=&#34;check package&#34; /&gt;
&lt;img src=&#34;/img/check_progress.png&#34; alt=&#34;check progress&#34; /&gt;
Check result is not clean. There is a warning and a note. We’ll need to address them.
&lt;img src=&#34;/img/check_warning.png&#34; alt=&#34;check warning&#34; /&gt;
Let’s fix importFrom.
&lt;img src=&#34;/img/check_warning_fixed1.png&#34; alt=&#34;check import fixed&#34; /&gt;
Then let’s fix DESCRIPTION.
&lt;img src=&#34;/img/check_warning_fixed2.png&#34; alt=&#34;check description fixed&#34; /&gt;
Once all issues are addressed, we see the check result is clean.&lt;/p&gt;
&lt;p&gt;Let’s build the source package (no shortcut)
&lt;img src=&#34;/img/build_src.png&#34; alt=&#34;build source package&#34; /&gt;
&lt;img src=&#34;/img/build_src_progress.png&#34; alt=&#34;build source progress&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;share&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Share&lt;/h2&gt;
&lt;p&gt;Three different ways to share our new R package.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;If you are the only user of the R package, then no additional action is required. cmd-shift-B (install step) installed the development version of the package already.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you want to share your R package with co-workers, you can (sftp) copy the zipped/built source file to a common location and have you co-workers them install the package in their R session by install.packages(“/path/to/pkg”, repos = NULL)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/build_src_progress.png&#34; alt=&#34;Setup git&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Setup git&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;/img/restart_after_git.png&#34; alt=&#34;Restart after git&#34; /&gt;
&lt;img src=&#34;/img/git_pane.png&#34; alt=&#34;Git pane&#34; /&gt;
&lt;img src=&#34;/img/commit.png&#34; alt=&#34;commit&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;If you want to share your R package with total strangers out there, you can share all source files in github (or any other version control systems) and have them install the package in their R session by devtools::install_github(“user/repo”)&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

        
      </description>
    </item>
    
    <item>
      <title>Random Forest Revisited</title>
      <link>/post/2018/05/11/random-forest-revisited/</link>
      <pubDate>Fri, 11 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/05/11/random-forest-revisited/</guid>
      <description>
        &lt;div id=&#34;hypothetical-setting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hypothetical setting&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Objective: binary classification&lt;/li&gt;
&lt;li&gt;N = 5M observations&lt;/li&gt;
&lt;li&gt;p = 10 variables (5 categorical and 5 continuous variables)&lt;/li&gt;
&lt;li&gt;ntree = 100 (model will not be adversely affected if ntree is too big)&lt;/li&gt;
&lt;li&gt;mtry = number of predictors to consider at each split (fixed or determined by resampling (10-fold cv)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;bootstrap-samples-for-each-tree&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bootstrap samples for each tree&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Bootstrap samples (sampling with replacement) of same size as the original data (N) is taken at each tree.&lt;/li&gt;
&lt;li&gt;This results in about 1/3 of N (hence approx. 1.6M) samples never being chosen for each tree. This is called out of bag (OOB) samples.&lt;/li&gt;
&lt;li&gt;This OOB samples are used for:
&lt;ul&gt;
&lt;li&gt;Unbiased estimate of test set error
&lt;ul&gt;
&lt;li&gt;Theoretically, there’s no need for cv or a separate test set to get an unbiased estimate of the test set error, as it’s estimated internally during tree constructions.&lt;/li&gt;
&lt;li&gt;OOB error estimate is the proportion of times (over all x_is in overall OOB samples) that predicted class is different from the true class for sample x&lt;sub&gt;i&lt;/sub&gt;. Note the prediction for x&lt;sub&gt;i&lt;/sub&gt; is obtained using only the trees that did not have x&lt;sub&gt;i&lt;/sub&gt; in bootstrap sample.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Estimate of variable importance: see variablem importance section below.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;split-criteria&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Split criteria&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;At each split, the mtry number of predictors are randomly selected (to de-correlate the trees), and a splitting variable is obtained.
&lt;ul&gt;
&lt;li&gt;The spliting variable is the one that results in the most homogeneous descendents (nodes).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Once the best mtry is fixed (or obtained via resampling, such as cv), caret (randomForest too?) fits the final model using the best mtry and save to finalModel.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;categorical-variable-treatment-various-encoding-methods&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Categorical variable treatment (various encoding methods)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Has implications in variable importance!
&lt;ul&gt;
&lt;li&gt;How to “aggregate” one-hot encoded feature importance!?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Has implications in grid search step!
&lt;ul&gt;
&lt;li&gt;Grid search for best mtry (in k-fold cv) might result in different result for mtry&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Various encoding methods
&lt;ul&gt;
&lt;li&gt;Label encoding (integer encoding/string indexing)
&lt;ul&gt;
&lt;li&gt;Simple hashing/lookup (dog –&amp;gt; 0, cat –&amp;gt; 1, etc.)&lt;/li&gt;
&lt;li&gt;If not the categorical variable is not ordinal, arbitrary indexing can be problematic (e.g., why dog (=0) &amp;lt; cat (=1)).
&lt;ul&gt;
&lt;li&gt;Python/Pyspark solves this issue by providing an ordering structure (i.e., 0 to the most frequent item, 1 to the second frequent, and so on).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;One-hot encoding (dummy variable)
&lt;ul&gt;
&lt;li&gt;Default method in caret (seems for both character and factor variables)
&lt;ul&gt;
&lt;li&gt;Default method in R maybe?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Is one-hot encoding bad? See [&lt;a href=&#34;http://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/&#34; class=&#34;uri&#34;&gt;http://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Binary encoding
&lt;ul&gt;
&lt;li&gt;Seems interesting, but see this:&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;continuous-variable-treatment-binning-vs.raw&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Continuous variable treatment (binning vs. raw)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;For customers subscription based analyses, it seems customary to bin some continuous variables such as tenures into bins, instead of using raw numbers.
&lt;ul&gt;
&lt;li&gt;Does this make your model rather robust?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;variable-importance-interpretations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Variable importance interpretations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;From the original randomForest package (and not from caret)
&lt;ul&gt;
&lt;li&gt;Mean decrease in accuracy (by permutation test, type = 1)
&lt;ul&gt;
&lt;li&gt;How’s permutation done?&lt;/li&gt;
&lt;li&gt;Randomly permuting the values of each predictor for the OOB sample of one predictor at a time for each tree.&lt;/li&gt;
&lt;li&gt;The difference in predictive performance between the non-permted sample and the permuted sample for each preditor is recorded and aggregated acoross the entire forest.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Mean decrease in impurity (no need for additional test, type = 2): total decrease in impurities resulting from using variable k as a splitter, averaged over all trees.
&lt;ul&gt;
&lt;li&gt;“… is often very consistent with the permutation importance measure”, from [&lt;a href=&#34;https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#varimp&#34; class=&#34;uri&#34;&gt;https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#varimp&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;What am I getting from caret::varImp vs. randomForest::importance?
&lt;ul&gt;
&lt;li&gt;By default, it seems both caret::varImp and randomForest::importance gives the impurity measure (type = 2)&lt;/li&gt;
&lt;li&gt;randomForest::importance(caret_model_object&lt;span class=&#34;math inline&#34;&gt;\(fit\)&lt;/span&gt;finalModel) gives the MeanDescreaseGini importance values, and so does caret::varImp(caret_model_object$fit, scale = FALSE)!!&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Variable selection/importance quality?
&lt;ul&gt;
&lt;li&gt;Relevant to different categorical encoding method (e.g., one-hot encoding results in individual level vs. label encoding just one)&lt;/li&gt;
&lt;li&gt;See Strobl et al. [&lt;a href=&#34;https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-25&#34; class=&#34;uri&#34;&gt;https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-25&lt;/a&gt;]
&lt;ul&gt;
&lt;li&gt;RF favors continuous variables and categoricals with many levels
&lt;ul&gt;
&lt;li&gt;So if all variables are categoricals with relatively small number of levels, prob ok for now&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;class-imbalance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Class imbalance&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;See &lt;a href=&#34;https://dpmartin42.github.io/posts/r/imbalanced-classes-part-1&#34; class=&#34;uri&#34;&gt;https://dpmartin42.github.io/posts/r/imbalanced-classes-part-1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

        
      </description>
    </item>
    
    <item>
      <title>Vim mode in RStudio</title>
      <link>/post/2018/05/11/vim-mode-in-rstudio/</link>
      <pubDate>Fri, 11 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/05/11/vim-mode-in-rstudio/</guid>
      <description>
        &lt;p&gt;I heard about Vim mode in RStudio but haven’t really given it a try. While working on a new blog post (not this one), I decided to give it a try, as I had to keep switching between non-Vim mode on my local Mac and Vim mode in my remote server (Linux). I’m not a Vim expert by any measure, but for some reason, I like working in Vim.&lt;/p&gt;
&lt;p&gt;So the option is in Tools/Global Options/Code/Key Bindings, and boom, I started using Vim in RStudio. One thing I ran into immediately, though, was that the cursor movement keys (h/j/k/l) wouldn’t repeat themselves when I kept them pressed. At first I thought maybe it’s one of those things that I’d have to live with, but quickly it became rather uncomfortable having to repeatedly pressing j/k keys to go up and down.&lt;/p&gt;
&lt;p&gt;Turns out all I had to do was to change system reference (on my Mac) as explained in this &lt;a href=&#34;https://support.rstudio.com/hc/en-us/articles/204896737&#34;&gt;support page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I feel like there must be some more (unexpected) additional features that come with Vim mode in RStudio (not sure if it’s intended or not), such as file save by cmd-s, which I find really helpful, because I don’t have to do esc-:-w just to save file :)&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>rJava and Mac OS</title>
      <link>/post/2018/05/03/rjava-and-mac-os/</link>
      <pubDate>Thu, 03 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/05/03/rjava-and-mac-os/</guid>
      <description>
        &lt;p&gt;For the past couple of days, I needed to install rJava package on my Mac, and boy did I know how complicated the process would turn out to be! I vaguely knew about the mess that is java and Mac OS, but I didn’t care that much until now, because (1) most of my development work has been done in my company environment (Linux) which I ssh into and is pretty well managed by folks more qualified to do so than me and (2) for my personal work (i.e., blogging, R package development, etc.), I didn’t need to touch java so far (or so I think).&lt;/p&gt;
&lt;p&gt;Then I thought of writing a wrapper function that makes a connection from within R to a company database&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. I had a snippet of code that does the job, and I decided to turn it into an R function and include it to my personal R package. Among other things, the code required rJava package, and that’s how I entered this mess that is java and Mac OS.&lt;/p&gt;
&lt;p&gt;Since I’m no expert there, I’m not going to try to explain each step I took, but basically, I searched for similar problems and solution, and I ended up following the awesome instruction given &lt;a href=&#34;http://www.owsiak.org/r-3-4-rjava-macos-and-even-more-mess/&#34;&gt;here&lt;/a&gt;. I should’ve taken screenshots of the error/ok messages that I got through this whole process, but long story short, I accidentally dropped “lib” at the end of this line where LDFLAGS&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; is being set, and it took a while to figure out what I was doing wrong:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;LDFLAGS = -L/usr/local/lib -L/Users/user_name/opt/clang+llvm-4.0.1-x86_64-apple-macosx10.9.0/lib -lomp&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once I fixed the error, rJava was installed successfully, and the database connection wrapper function is added to the personal package. Special thanks to the &lt;a href=&#34;http://www.owsiak.org/about-author/&#34;&gt;author&lt;/a&gt; of the above blog post and the &lt;a href=&#34;http://urbanek.info/proj.php&#34;&gt;author&lt;/a&gt; of the rJava package!&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Making a database connection should be doable via the Connections pane in RStudio, but let’s assume it can’t be used for whatever reason.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Don’t even know what it is, but I’m ok with that for now.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

        
      </description>
    </item>
    
    <item>
      <title>Use of quos</title>
      <link>/post/2018/04/20/use-of-quos/</link>
      <pubDate>Fri, 20 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/04/20/use-of-quos/</guid>
      <description>
        &lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressPackageStartupMessages(library(uncmbb))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(jutilr))

head(unc) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Season  Game_Date Game_Day Type Where   Opponent_School Result Tm Opp
## 1   1950 1949-12-01      Thu  REG     H              Elon      W 57  39
## 2   1950 1949-12-03      Sat  REG     A          Richmond      W 58  50
## 3   1950 1949-12-05      Mon  REG     A     Virginia Tech      L 48  62
## 4   1950 1949-12-07      Wed  REG     A      Lenoir-Rhyne      L 78  79
## 5   1950 1949-12-09      Fri  REG     H George Washington      L 44  54
## 6   1950 1949-12-28      Wed  REG     N     West Virginia      L 50  58
##     OT
## 1 &amp;lt;NA&amp;gt;
## 2 &amp;lt;NA&amp;gt;
## 3 &amp;lt;NA&amp;gt;
## 4   OT
## 5 &amp;lt;NA&amp;gt;
## 6 &amp;lt;NA&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot1 &amp;lt;- function(lst, x, y){
  
  x &amp;lt;- enquo(x)
  y &amp;lt;- enquo(y)
  
  x_name &amp;lt;- quo_name(x)
  y_name &amp;lt;- quo_name(y)
  
  lst %&amp;gt;% map(function(df) df %&amp;gt;% 
                ggplot(aes_string(x = paste0(&amp;quot;reorder(&amp;quot;, x_name, &amp;quot;, -&amp;quot;, y_name, &amp;quot;)&amp;quot;), y = y_name)) +
                geom_bar(stat = &amp;quot;identity&amp;quot;) +
                geom_text(aes(label = n), vjust = -0.2) +
                theme_bw() +
                scale_y_continuous(label = scales::percent) +
                labs(title = paste0(&amp;quot;Frequency of &amp;quot;, x_name)))

}

teams &amp;lt;- list(unc, duke)

get_freq &amp;lt;- function(df, var){
  var &amp;lt;- enquo(var)
  df %&amp;gt;% count(!!var) %&amp;gt;% 
          arrange(desc(n)) %&amp;gt;%
          add_percent(var = &amp;quot;n&amp;quot;)
}
teams %&amp;gt;% map(function(df) df %&amp;gt;% get_freq(Opponent_School) %&amp;gt;% head)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## # A tibble: 6 x 3
##   Opponent_School          n   perc
##   &amp;lt;chr&amp;gt;                &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 Duke                   179 0.0817
## 2 North Carolina State   167 0.0762
## 3 Wake Forest            155 0.0707
## 4 Maryland               142 0.0648
## 5 Clemson                140 0.0639
## 6 Virginia               140 0.0639
## 
## [[2]]
## # A tibble: 6 x 3
##   Opponent_School          n   perc
##   &amp;lt;chr&amp;gt;                &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 North Carolina         179 0.0818
## 2 Wake Forest            173 0.0790
## 3 North Carolina State   163 0.0745
## 4 Maryland               146 0.0667
## 5 Virginia               138 0.0630
## 6 Clemson                127 0.0580&lt;/code&gt;&lt;/pre&gt;

        
      </description>
    </item>
    
  </channel>
</rss>
