<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jay&#39;s Notes</title>
    <link>/</link>
    <description>Recent content on Jay&#39;s Notes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 13 Jan 2019 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Coping with worst loss at home</title>
      <link>/post/2019/01/13/worst-performance-at-home-sweet-dome/</link>
      <pubDate>Sun, 13 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/01/13/worst-performance-at-home-sweet-dome/</guid>
      <description>
        &lt;p&gt;It’s been a tough weekend, not least because Tar Heels lost at home. Sometimes I feel like I’m vested too much in the outcome of the Heel’s basketball games, and if my emotional rollercoaster the rest of the weekend gives any clue, I probably am, just a litttle bit. I tried to shake it off, yet it wasn’t particularly easy, not only because we lost at home, but because the loss was the worst one at home under Roy Williams. So I turn to blogging, after 100+ days of hiatus, as a last resort to restoring my inner peace and calm, before the new workday begins.&lt;/p&gt;
&lt;p&gt;First I look at a couple of worst of the worst losses at home (to make the last loss less painful).&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;10 worst home loss since 1949-50 season
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Season
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Game_Date
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Opponent_School
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Tm
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Opp
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
lost_by
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1964
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1964-02-29
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Duke
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
69
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
104
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
35
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2002
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2002-01-31
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Duke
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
58
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
87
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1951
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1950-12-20
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Eastern Kentucky
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
62
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
85
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1955
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1955-02-04
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Duke
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
68
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
91
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1975
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1975-02-15
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Maryland
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
74
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
96
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
22
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2002
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2002-01-05
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Wake Forest
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
62
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
84
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
22
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1953
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1953-02-21
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
North Carolina State
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
66
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
87
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1999
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1999-02-27
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Duke
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
61
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
81
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2002
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2002-01-23
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
North Carolina State
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
59
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
77
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1970
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1970-02-21
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
South Carolina
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
62
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
79
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Sure enough, 4 losses to Duke (gee, a loss by 35!) and a couple to State (a loss by 21, which matches the weekend’s loss margin). At this point, I’m realizing this is not particularly going to help me feel better about the weekend’s loss. Worse, it might exacerbate the wound, but I decide to keep going. What’s all the home losses look like?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-13-worst-performance-at-home-sweet-dome_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;1152&#34; /&gt;&lt;img src=&#34;/post/2019-01-13-worst-performance-at-home-sweet-dome_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Turns out, of the 122 total home losses (since 1949-50 season), loss by 4 is the most frequent (14 games), followed by loss by 1 (13 games). Sure, Duke won the most with 25 wins against UNC, followed by State (16), Wake Forest (15), and Maryland (13). Other than these 4 schools, no schools have won at UNC more than 6 times since 1949-50 season. I’m already starting to feel a little better. While at it, I decide to look at how Duke’s home loss look like for the heck of it.&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-3&#34;&gt;Table 2: &lt;/span&gt;10 worst home loss since 1949-50 season
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Season
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Game_Date
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Opponent_School
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Tm
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Opp
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
lost_by
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1959
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1958-12-29
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Michigan State
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
57
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
82
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
25
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1960
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1960-02-27
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
North Carolina
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
75
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
25
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1973
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1973-02-21
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
North Carolina State
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
74
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
24
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1975
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1975-01-27
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
North Carolina State
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
71
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
95
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
24
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1975
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1975-02-08
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Maryland
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
80
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
104
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
24
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1983
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1983-03-05
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
North Carolina
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
81
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
105
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
24
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1989
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1989-01-18
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
North Carolina
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
71
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
91
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1960
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1960-02-20
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Wake Forest
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
64
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
83
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
19
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1974
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1974-01-23
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Wake Forest
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
71
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
90
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
19
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1950
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1950-02-24
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
North Carolina
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
46
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
64
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-13-worst-performance-at-home-sweet-dome_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;1152&#34; /&gt;&lt;img src=&#34;/post/2019-01-13-worst-performance-at-home-sweet-dome_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Turns out, of the 135 Duke’s home losses (since 1949-50 season), Carolina beat them 29 times, followed by State (21), Wake Forest (15), Maryland (14), and Virginia (9). Other than these 5 schools, no schools have beat Duke more than 6 times since 1949-50 season.&lt;/p&gt;
&lt;p&gt;Another way to look at how &lt;a href=&#34;https://joongsup.rbind.io/post/2018/08/05/home-sweet-dome/&#34;&gt;sweet&lt;/a&gt; home games have been since Roy Williams came home might be to look at how rare the home losses have been.&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-4&#34;&gt;Table 3: &lt;/span&gt;Home loss since 2003-04 season
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Season
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Game_Date
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Opponent_School
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Tm
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Opp
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
lost_by
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2013
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2013-03-09
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Duke
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
53
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
69
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2010
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2010-01-31
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Virginia
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
75
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2010
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2010-01-20
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Wake Forest
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
69
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
82
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
13
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015-02-24
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
North Carolina State
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
46
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
58
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2006
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2006-01-14
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Miami
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
70
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
81
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
11
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2008
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2008-02-06
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Duke
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
78
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
89
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
11
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015-02-02
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Virginia
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
64
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
75
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
11
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2010
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2010-02-10
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Duke
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
54
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
64
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2010
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2010-02-24
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Florida State
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
67
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
77
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2013
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2013-01-10
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Miami
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
59
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
68
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2006
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2006-01-25
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Boston College
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
74
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
81
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2009
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2009-01-04
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Boston College
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
78
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
85
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015-03-07
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Duke
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
77
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
84
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2014
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2014-01-08
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Miami
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
57
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
63
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2004
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2003-12-20
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Wake Forest
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
114
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
119
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2014-12-03
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Iowa
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
55
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2006
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2005-11-29
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Illinois
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
64
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
68
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2006
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2006-02-07
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Duke
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
83
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
87
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-12-20
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Wofford
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
75
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
79
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-01-27
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
North Carolina State
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
91
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
95
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2014
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2013-11-17
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Belmont
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
80
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
83
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2014
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2013-12-18
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Texas
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
83
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
86
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-02-27
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Miami
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
88
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
91
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2004
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2004-02-05
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Duke
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
81
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
83
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2008
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2008-01-19
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Maryland
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
80
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
82
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2010
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2010-01-16
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Georgia Tech
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
71
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
73
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2007
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2007-02-13
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Virginia Tech
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
80
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
81
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2012
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2012-02-08
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Duke
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
84
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
85
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015-01-05
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Notre Dame
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
70
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
71
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2016
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2016-02-17
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Duke
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
73
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
74
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Ok, a good start. There’s been only 30 home losses since 2003-04 season, approximately 2 home losses per season. Only 2 down days over a 4 months period each season. Put it that way, I feel like I can survive this weekend’s L.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-13-worst-performance-at-home-sweet-dome_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;1152&#34; /&gt;&lt;img src=&#34;/post/2019-01-13-worst-performance-at-home-sweet-dome_files/figure-html/unnamed-chunk-5-2.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Since 2003-04 season, again loss by 1 and 4 were most frequent, each with 4 games. Not surprisingly, Duke has won 8 games at UNC, and somewhat surprisingly, Miami has second most wins at UNC since RW came home. This weekend’s win gives Louisville its first win at Carolina, it looks like.&lt;/p&gt;
&lt;p&gt;In addition to the worst home loss in 16 years, what made things worse for me personally was the fact that Duke won their game against Florida State later same day. Again, this is just me, but after the home loss, I was begging for Florida State to win their home game, so that I can have somewhat so-so weekend even with the home loss, even though I knew how &lt;a href=&#34;https://joongsup.rbind.io/post/2018/01/07/same-day-match-results-unc-and-duke/&#34;&gt;rare&lt;/a&gt; such event was.&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-6&#34;&gt;Table 4: &lt;/span&gt;Carolina L and Duke W on same day since 1949-50 season
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Season
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Game_Date
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Opponent_School
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Tm
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Opp
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
lost_by
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-12-20
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Wofford
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
75
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
79
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2014-12-03
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Iowa
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
55
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2009
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2009-01-04
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Boston College
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
78
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
85
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2008
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2008-01-19
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Maryland
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
80
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
82
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2006
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2006-01-14
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Miami
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
70
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
81
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
11
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2004
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2003-12-20
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Wake Forest
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
114
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
119
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2003
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2002-12-07
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Kentucky
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
81
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
98
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2002
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2001-11-20
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Davidson
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
54
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
58
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2001
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2000-12-02
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Kentucky
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
76
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
93
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2000-01-22
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Florida State
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
71
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
76
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1999
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1999-01-13
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Maryland
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
76
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
89
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
13
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1989
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1989-01-07
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Iowa
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
97
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
98
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1970
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1970-02-21
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
South Carolina
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
62
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
79
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1968
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1968-02-28
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
South Carolina
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
86
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
87
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1966
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1966-02-12
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Virginia Tech
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
75
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
81
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1966
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1965-12-31
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
West Virginia
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
97
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
102
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1963
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1963-02-09
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Wake Forest
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
71
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
72
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1962
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1962-02-10
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Wake Forest
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
80
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
87
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1962
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1961-12-11
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Indiana
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
70
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
76
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1953
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1953-02-21
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
North Carolina State
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
66
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
87
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1952
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1952-01-26
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
North Carolina State
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
53
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
58
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1950
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1950-02-07
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Wake Forest
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
54
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
57
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Ok, 22 such days when Carolina lost at home while Duke won on the same day since 1949-50 season. And only 6 such days since 2003-04 season. Make it 7 with this weekend’s case, but nevertheless at the end of the day, an L is an L, and we had two road Ws before this L, and I hope the team shook it off well over the weekend. Writing this post definitely helped me feel a whole lot better about this weekend and get ready for a new day tomorrow! Speaking of which, another same day games tomorrow, Notre Dame at UNC and Syracuse at Duke. Let’s rebound from this weekend’s L Heels, and go get your 2nd W Cuse at Duke! That’ll be sweet indeed :)&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>First 8 games results</title>
      <link>/post/2018/12/01/first-8-games-results/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/12/01/first-8-games-results/</guid>
      <description>
        &lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load libraries for R

suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(uncmbb)) # dataset
n &amp;lt;- 8
df_n &amp;lt;- unc %&amp;gt;% group_by(Season) %&amp;gt;% 
             arrange(Game_Date) %&amp;gt;%
             top_n(-n, Game_Date)


a &amp;lt;- df_n %&amp;gt;% group_by(Season) %&amp;gt;%
          summarize(wins = sum(Result == &amp;quot;W&amp;quot;), losses = sum(Result == &amp;quot;L&amp;quot;)) %&amp;gt;%
          left_join(mbb_champ_season(unc), by = &amp;quot;Season&amp;quot;) 

b &amp;lt;- a %&amp;gt;% ggplot(aes(x = losses)) +
            geom_bar()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_pre12 &amp;lt;- unc %&amp;gt;% group_by(Season) %&amp;gt;%
                  filter(as.numeric(substring(Game_Date, 6, 7)) %in% c(10, 11)) %&amp;gt;%
                  summarize(games = n(), wins = sum(Result == &amp;quot;W&amp;quot;), losses = sum(Result == &amp;quot;L&amp;quot;))

b &amp;lt;- df_pre12 %&amp;gt;% ggplot(aes(x = losses)) + 
              geom_bar()&lt;/code&gt;&lt;/pre&gt;

        
      </description>
    </item>
    
    <item>
      <title>SQL, Visualization, R, and Python</title>
      <link>/post/2018/11/03/plotting-101-r-vs-python/</link>
      <pubDate>Sat, 03 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/11/03/plotting-101-r-vs-python/</guid>
      <description>
        &lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#setup&#34;&gt;Setup&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#load-libraries&#34;&gt;Load libraries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#read-data&#34;&gt;Read data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bar-chart&#34;&gt;Bar chart&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#simple-frequency&#34;&gt;Simple frequency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#grouped-frequency&#34;&gt;Grouped frequency&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#line-chart&#34;&gt;Line chart&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#simple-sequence&#34;&gt;Simple sequence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#grouped-sequence&#34;&gt;Grouped sequence&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Not long ago, I saw a thread in my tweeter feed that was talking about what should be the first thing that a data scientist student learns. There were two answers that made sense to me: SQL and visualization.&lt;/p&gt;
&lt;p&gt;SQL, because, one needs to be able to get desired data from somewhere, and for the most part in an industry setting at least, that somewhere is typically some form of databases (e.g., relational database, hive, etc.), which can be accessed by SQL-like languages. As far as basics go, I think being able to select data (with conditions) and create/drop table should carry one surprisingly a long way in a data science journey.&lt;/p&gt;
&lt;p&gt;Visualizaton, because, one needs to be able to show acquired data and analyses done on them in some kind of non-text form, and that non-text form typically involves graphs. As far as basics go, I think being able to show data on a bar chart and a line chart should carry one surprisingly a long way in a data science journey.&lt;/p&gt;
&lt;p&gt;About the same time, I started using Python a bit more at work, and with the twitter thread fresh in mind, I thought this might be a good time to write a post that compares how to do frequently used plotting in both R and Python. By frequently used plotting, I mean bar charts and line charts based on personal experience. I’ve been using bar chart usually for frequency of categorical variables and line chart for sequence of numerical variables.&lt;/p&gt;
&lt;p&gt;In this post, I’ll try to provide as minimal codes as possible to draw two flavors in each: simple chart and grouped chart.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- Bar plot for frequency of categorical variable
- line plot for sequence of numerical variable&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;R vs. Python
- Similarities
- Differences&lt;/p&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Setup&lt;/h1&gt;
&lt;div id=&#34;load-libraries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Load libraries&lt;/h2&gt;
&lt;div id=&#34;r&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load libraries for R

suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(uncmbb)) # dataset&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;python&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Python&lt;/h3&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Load libraries for Python
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;read-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Read data&lt;/h2&gt;
&lt;div id=&#34;r-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# no need to add uncmbb::, but to clarify where object &amp;#39;unc&amp;#39; is coming from
unc_r &amp;lt;- uncmbb::unc 
print(head(unc_r))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Season  Game_Date Game_Day Type Where   Opponent_School Result Tm Opp
## 1   1950 1949-12-01      Thu  REG     H              Elon      W 57  39
## 2   1950 1949-12-03      Sat  REG     A          Richmond      W 58  50
## 3   1950 1949-12-05      Mon  REG     A     Virginia Tech      L 48  62
## 4   1950 1949-12-07      Wed  REG     A      Lenoir-Rhyne      L 78  79
## 5   1950 1949-12-09      Fri  REG     H George Washington      L 44  54
## 6   1950 1949-12-28      Wed  REG     N     West Virginia      L 50  58
##     OT
## 1 &amp;lt;NA&amp;gt;
## 2 &amp;lt;NA&amp;gt;
## 3 &amp;lt;NA&amp;gt;
## 4   OT
## 5 &amp;lt;NA&amp;gt;
## 6 &amp;lt;NA&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;python-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Python&lt;/h3&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# read R data object (from UNCMBB package) in Python
unc_py = r.unc
print(unc_py.head())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Season  Game_Date Game_Day Type ... Result    Tm   Opp  OT
## 0   1950 1949-12-01      Thu  REG ...      W  57.0  39.0  NA
## 1   1950 1949-12-03      Sat  REG ...      W  58.0  50.0  NA
## 2   1950 1949-12-05      Mon  REG ...      L  48.0  62.0  NA
## 3   1950 1949-12-07      Wed  REG ...      L  78.0  79.0  OT
## 4   1950 1949-12-09      Fri  REG ...      L  44.0  54.0  NA
## 
## [5 rows x 10 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;bar-chart&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Bar chart&lt;/h1&gt;
&lt;div id=&#34;simple-frequency&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simple frequency&lt;/h2&gt;
&lt;div id=&#34;r-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Option 1: no pre summarizing
unc_r %&amp;gt;% ggplot(aes(x = Type)) +
          geom_bar()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-03-plotting-101-r-vs-python_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Option 2: summarize data before plotting
unc_r %&amp;gt;% group_by(Type) %&amp;gt;%
          summarize(n = n()) %&amp;gt;% # see ?dplyr::count
          ggplot(aes(x = Type, y = n)) +
          geom_bar(stat = &amp;quot;identity&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-03-plotting-101-r-vs-python_files/figure-html/unnamed-chunk-5-2.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;python-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Python&lt;/h3&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Option 1: no pre summarizing
unc_py[&amp;#39;Type&amp;#39;].value_counts().plot(kind = &amp;#39;bar&amp;#39;)
plt.show()
# Option 2: summarize data before plotting&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-03-plotting-101-r-vs-python_files/figure-html/unnamed-chunk-6-1.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;unc_py.groupby(&amp;#39;Type&amp;#39;).size().plot(kind = &amp;#39;bar&amp;#39;)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-03-plotting-101-r-vs-python_files/figure-html/unnamed-chunk-6-2.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;grouped-frequency&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Grouped frequency&lt;/h2&gt;
&lt;div id=&#34;r-3&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;unc_r %&amp;gt;% group_by(Type, Result) %&amp;gt;%
          summarize(n = n()) %&amp;gt;%
          ggplot(aes(x = Type, y = n)) +
          geom_bar(stat = &amp;quot;identity&amp;quot;, position = &amp;quot;dodge&amp;quot;, aes(fill = factor(Result)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-03-plotting-101-r-vs-python_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;python-3&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Python&lt;/h3&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;t = unc_py.groupby([&amp;#39;Type&amp;#39;, &amp;#39;Result&amp;#39;]).size().reset_index(name = &amp;#39;games&amp;#39;).pivot(&amp;#39;Type&amp;#39;, &amp;#39;Result&amp;#39;, &amp;#39;games&amp;#39;).reset_index()
t.plot(kind = &amp;#39;bar&amp;#39;, x = &amp;#39;Type&amp;#39;)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-03-plotting-101-r-vs-python_files/figure-html/unnamed-chunk-8-1.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;line-chart&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Line chart&lt;/h1&gt;
&lt;div id=&#34;simple-sequence&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simple sequence&lt;/h2&gt;
&lt;div id=&#34;r-4&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Option 1: use join (to divide # of wins by # of games)
t &amp;lt;- unc_r %&amp;gt;% count(Season)
unc_r %&amp;gt;% group_by(Season, Result) %&amp;gt;%
            summarize(games = n()) %&amp;gt;%
            inner_join(t, by = &amp;quot;Season&amp;quot;) %&amp;gt;%
            filter(Result == &amp;quot;W&amp;quot;) %&amp;gt;%
            mutate(perc = round(games/n, 4)) %&amp;gt;%
            ggplot(aes(x = Season, y = perc)) + 
            geom_line(aes(group = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-03-plotting-101-r-vs-python_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Option 2: using jutilr::add_percent
unc_r %&amp;gt;% group_by(Season, Result) %&amp;gt;%
            summarize(games = n()) %&amp;gt;%
            jutilr::add_percent(var = &amp;quot;games&amp;quot;) %&amp;gt;% 
            filter(Result == &amp;quot;W&amp;quot;) %&amp;gt;%
            ggplot(aes(x = Season, y = perc)) + 
            geom_line(aes(group = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-03-plotting-101-r-vs-python_files/figure-html/unnamed-chunk-9-2.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;python-4&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Python&lt;/h3&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Optioin 1: use join (to divide # of wins by # of games per season)
t = unc_py.Season.value_counts().reset_index(name = &amp;quot;total&amp;quot;)
t2 = unc_py.groupby([&amp;#39;Season&amp;#39;, &amp;#39;Result&amp;#39;]).size().reset_index(name = &amp;#39;games&amp;#39;)
t3 = pd.merge(t, t2, left_on = &amp;#39;index&amp;#39;, right_on = &amp;#39;Season&amp;#39;).sort_values(&amp;#39;Season&amp;#39;)
t3[&amp;#39;perc&amp;#39;] = t3.games/t3.total
t3.loc[t3.Result == &amp;#39;W&amp;#39;, [&amp;#39;Season&amp;#39;, &amp;#39;perc&amp;#39;]].plot(kind = &amp;#39;line&amp;#39;, x = &amp;#39;Season&amp;#39;, y = &amp;#39;perc&amp;#39;)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-03-plotting-101-r-vs-python_files/figure-html/unnamed-chunk-10-1.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;grouped-sequence&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Grouped sequence&lt;/h2&gt;
&lt;div id=&#34;r-5&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;unc_r %&amp;gt;% group_by(Season, Result) %&amp;gt;%
            summarize(games = n()) %&amp;gt;%
            ggplot(aes(x = Season, y = games)) + 
            geom_line(aes(group = Result, colour = Result))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-03-plotting-101-r-vs-python_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;python-5&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Python&lt;/h3&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;t = unc_py.groupby([&amp;#39;Season&amp;#39;, &amp;#39;Result&amp;#39;]).size().reset_index(name = &amp;#39;games&amp;#39;).pivot(&amp;#39;Season&amp;#39;, &amp;#39;Result&amp;#39;, &amp;#39;games&amp;#39;).reset_index()
t.plot(kind = &amp;#39;line&amp;#39;, x = &amp;#39;Season&amp;#39;)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-03-plotting-101-r-vs-python_files/figure-html/unnamed-chunk-12-1.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

        
      </description>
    </item>
    
    <item>
      <title>Home game streak</title>
      <link>/post/2018/10/06/home-game-streak/</link>
      <pubDate>Sat, 06 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/10/06/home-game-streak/</guid>
      <description>
        &lt;p&gt;The schedule for the 2018-2019 UNC MBB season has been out, and I ran into a blog post that said something about some teams getting preferable schedules than others, in that those lucky teams get to play three consecutive games at home and in some case, not having to face the same number of consecutive games on the road. I remembered UNC team had a touch streak last season, having to play 3 in a row on the road, and with the current 15 member system (since 2010), such imbalance in schedule is bound to happen eventually.&lt;/p&gt;
&lt;p&gt;So I was curious in terms of playing consecutive home games and road games, who’s been lucky all these years. Let’s see.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Vim, vim-slime, and screen</title>
      <link>/post/2018/10/06/vim-and-slime/</link>
      <pubDate>Sat, 06 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/10/06/vim-and-slime/</guid>
      <description>
        &lt;p&gt;As much as I love using RStudio for everything&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, there are times when I can’t really use it, i.e., for some projects at work in which I need to log in to some servers and do work there (i.e., on the server side). Using RStudio server would do it, I think, but it’s not an option at the moment. For a long time, I’ve been trying to establish a good working data analysis workflow that I can follow in those situations.&lt;/p&gt;
&lt;p&gt;Vim is my server side editor of choice. I’m not a Vim expert by no means, but ever since I started using it (it’s been almost 4 years), it fit me, and it’s been my editor of choice, especially when working on some remote servers&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. I would open up several terminal window tabs, i.e. one for command line, one for R session, one for file editing (using Vim), one for hive shell, etc..Establishing an effective data analysis workflow came down to being able to use Vim more seamlessly with those other tools that I use everyday.&lt;/p&gt;
&lt;p&gt;After several days of browsing, reading, and trying out, I finally settled down with this particular set-up and so far I’m pretty happy with it. This post is a quick overview of the set-up as a reference for myself and potentially others who’s facing a similar situation. This is one of those things that I wish I had perfected while I was still at school, just like many other things that I’ve written about before here.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Vim and .vimrc&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I searched web for information regarding Vim set up for data analysis, especially using R. Very quickly, I ran into &lt;a href=&#34;https://github.com/jalvesaq/Nvim-R&#34;&gt;Nvim-R&lt;/a&gt;, a Vim plugin to edit R code. I tried it for a couple of days, but ended up dropping it, because as much as I love R, I had to use some other tools with Vim, and I needed more general solution, not an R-specific solution.&lt;/p&gt;
&lt;p&gt;But that turned out to be my introduction to the world of Vim plugins! I spent a couple of nights searching for cool Vim plugins, sometimes for the ones that are not necessarily related to doing data analysis. But it was so fun trying out a variety of Vim plugins. This is also when I started using Vim plugin manager as well, which makes installing/uninstalling Vim plugins simple and easy. I tried only one such manager, &lt;a href=&#34;https://github.com/VundleVim/Vundle.vim&#34;&gt;Vundle&lt;/a&gt;, and have been using it ever since.&lt;/p&gt;
&lt;p&gt;Now to use a Vim plugin manager and several plugins, I started adding more stuffs in my .vimrc file, which is like a .Rprofile file (or .bashrc). It contains many settings for Vim, i.e., plugins related specific setting as well as more general settings. And boy are there tons of cool stuffs you can do with these settings! Of the many general settings, one of my favorite is insert mode mapping of “jj” (two strokes of the key j) to esc key, so that I don’t have to reach for the esc key while typing, and instead just type jj to get back to normal mode&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. Like I said, I spent several couple of nights trying out different settings and plugins, and I had to resist the urge to play with new settings and plugins. It’s like trying to perfect how your blog looks like, and I had to try hard not to spend too much time perfecting my .vimrc file, i.e., my Vim setup.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Vim-slime&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then I ran into &lt;a href=&#34;https://github.com/jpalardy/vim-slime&#34;&gt;Vim-slime&lt;/a&gt;, another Vim plugin that allows you to send lines of code from one window to another, thus eliminating the need for constant copying from a text file (e.g., data analysis script file) and pasting on R session, for example. It’s the equivalence of selection and cmd-enter in RStudio to run select lines of code.&lt;/p&gt;
&lt;p&gt;With Vim-slime, I can send select lines of codes from one terminal window tab to another, be it an R console, Python console, Hive shell, etc. So it provided me with a more general solution than Nvim-R, which is still a great R specific plugin.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Screen&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Last but not least, I had to come up with a way to keep my running terminal sessions even after server connection is lost. I have several meetings a day, and sometimes I lose my server connection while walking to/from a meeting. I already knew and have been using &lt;a href=&#34;https://www.gnu.org/software/screen/&#34;&gt;screen&lt;/a&gt; for that purpose, and it so happens that Vim-slime works great with screen. By default, a chunk of code I want to run is sent to a screen session, and I learned how to name a screen session too.&lt;/p&gt;
&lt;p&gt;I also ran into &lt;a href=&#34;https://github.com/tmux/tmux&#34;&gt;tmux&lt;/a&gt;, which is a screen multiplexer, similar to GNU screen. Essentially just like screen, it lets you work on multiple tabs within one terminal window without losing them even after your server connection is lost. It looked very interesting and useful, but unfortunately, it needed to be installed on the server side, and to make things worse, the server was missing one of the required dependencies, so I decided to drop tmux from my target list.&lt;/p&gt;
&lt;p&gt;So putting these together, here’s what I typically do first thing in the morning at work. There’s probably a better way to do this still, but until I figure out how to, I’m happy with the current set up.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Open terminal and ssh into a server. Open the script file, e.g., an R script, that I need to work on. Let’s call this a “source” tab.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:source&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/slime_src.png&#34; alt=&#34;Source screenshot&#34; width=&#34;75%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Source screenshot
&lt;/p&gt;
&lt;/div&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Open another tab in the same terminal window (cmd-T) and ssh into the same server. Start a named screen session by &lt;code&gt;screen -S R&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;$screen -S R&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will open a screen sesion whose name is “R”. Once in it, start an R shell, (or python, hive, etc.), and let’s call it a “console” tab. I find it helpful to name each of these console tabs with corresponding tool name, e.g., “hive” when using hive shell there, “python” when using python shell there, and “R” when using R shell there, etc..&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;When working on some code chunks, test them by sending the code chunk from the “source” tab (from #1) to the “console” tab (from #2). Selecting code chunks in visual mode in Vim and sending them to the console screen by &lt;ctrl-c&gt;&lt;ctrl-c&gt; using Vim-slime takes some getting used to, but it’s been worth it :)&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:workflow&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/out.gif&#34; alt=&#34;Vim-slime workflow screenshots&#34; width=&#34;75%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Vim-slime workflow screenshots
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;When code chunks are sent for the first time in a session, Vim-slime asks for the destination screen session name, and I entered R, which was my screen session name from step 2 above. It then asks for screen window name, and I just leave it as is (default 0).&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Especially for blogging, yeah!&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;In fact, I’m using Vim mode in RStudio too.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;I wonder if there’s a way to map jj to esc in RStudio vim mode too.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

        
      </description>
    </item>
    
    <item>
      <title>GNU Make for Data Analysis Workflow Management</title>
      <link>/post/2018/08/26/gnu-make-for-workflow-manager/</link>
      <pubDate>Sun, 26 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/08/26/gnu-make-for-workflow-manager/</guid>
      <description>
        &lt;p&gt;I’ve finally started using GNU &lt;a href=&#34;https://www.gnu.org/software/make/&#34;&gt;make&lt;/a&gt; as a data analysis workflow management tool. I knew it existed as a software “build” tool, and although I always thought Makefile&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; sound pretty cool, I never actually had to use it, not just as a build tool, but also as a data analysis workflow management tool.&lt;/p&gt;
&lt;p&gt;It started with &lt;span class=&#34;citation&#34;&gt;@thosjleepr&lt;/span&gt;’s &lt;a href=&#34;https://twitter.com/thosjleeper/status/1030105885085970432?s=03&#34;&gt;tweet&lt;/a&gt; that showed up in my feed:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Inspired by &lt;span class=&#34;citation&#34;&gt;@carlislerainey&lt;/span&gt;, I’ve added some code to my intro #make tutorial that visualizes the makefile’s dependency graph natively in R using #ggraph.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I never actually looked closely at what the author did in this nice tutorial, but rather ended up taking a detour and actually started looking more closely into using make as a workflow management tool. It so happened that at the time, I needed to keep running some set of scripts over and over depending on “refresh” status of each step, and so when I ran into this tweet, I decided to jump on using make finally, and now make is one of those topics that I wish I had learned and started using while at school&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It’s been less than 2 weeks since I started using make, and I’m not a make expert by no means. Nonetheless, &lt;a href=&#34;https://joongsup.rbind.io/about/&#34;&gt;I like talking about data anlaysis workflow&lt;/a&gt;, so here’s my thoughts on using make as a data analysis workflow management tool as a reference. Since it’s my thoughts as of today, I’m sure some of my takes will change over time.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;It’s relevant to many data analysis projects.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;From the Wikipedia on &lt;a href=&#34;https://en.wikipedia.org/wiki/Make_(software)&#34;&gt;make (software)&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Besides building programs, Make can be used to manage any project where some files must be updated automatically from others whenever the others change.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It makes sense many tutorials on make (especially in the context of using it together with R) that I encountered used generating some documents as a use case. E.g., use make to streamline updating plots and inserting newly refreshed plots in the final document output. My immediate need didn’t involve updating a report over and over, but rather looked something like Figure &lt;a href=&#34;#fig:workflow&#34;&gt;1&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:workflow&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/makefile_workflow.png&#34; alt=&#34;Base Workflow&#34; width=&#34;40%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Base Workflow
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Basically this example workflow involves generating/refreshing several hive tables, training ML models, and joining hive tables. Sometimes all 5 steps need to be run in a proper sequence, other times only a subset of steps need to be run, like Figure &lt;a href=&#34;#fig:scenarios&#34;&gt;2&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:scenarios&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/makefile_ex_1.png&#34; alt=&#34;Workflow Scenarios&#34; width=&#34;40%&#34; /&gt;&lt;img src=&#34;/img/makefile_ex_3.png&#34; alt=&#34;Workflow Scenarios&#34; width=&#34;40%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Workflow Scenarios
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;So if there’s anyone out there who’s on the edge and haven’t started using a tool like make for workflow management, thinking such tool is not relevant in data analysis, I hope above figures (and this post) provide some convincing argument for using it.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;It helped me put more efforts in modularizing codes.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A Makefile is a collection of one or more rules, with a single rule consisting of a target, dependencies, and commands. In order to use commands in a Makefile, it’s imperative that the codes can be run in command line, and not just in interactive environment (REPL).&lt;/p&gt;
&lt;p&gt;I don’t know what’s the best way to write a “program” for a data analysis project, and many of my codes are still mostly used in interactive mode. With the use of make, however, I’m putting more efforts to make sure my codes are run in command line, and REPL is used only for checking snippet of codes, not the entirety. So instead of working exclusively in a REPL shell (say R and/or hive shell), I now try to make sure the script files work by starting them in command line.&lt;/p&gt;
&lt;p&gt;This is an important shift, I think, that has more implications, and I’m sure I’ll have more to say about this shift in the future.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;It helps keep track of workflow dependency and documentation in one place.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is a nice little benefit of using make that I liked nonetheless. When I work on a project for 2-3 days, take a break from it, and come back in 2-3 weeks, I want to be able to recall what I have done and start picking up on things with minimal effort. With a combination of comments and a sed command in a Makefile, helpful documentation/notes can be extracted from command line like below.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;~$ make help
 1_gen_tbl_A: generate/refresh table A
 2_train_model: train ml model using table A and generate table B
 3_gen_tbl_C: generate/refresh table C (not dependent on steps 1 and 2!)
 4_join_B_and_C: join tables B and C as a prep for step 5
 5_compare_B_and_C: compare B and C by creating a performance table
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Software carpentry’s &lt;a href=&#34;https://swcarpentry.github.io/make-novice/08-self-doc/index.html&#34;&gt;lesson&lt;/a&gt; on automation and make provides a how-to.&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;There are many alternatives to GNU make.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The tool has been around for nearly 40 years now. Obviously, I like it for several reasons as a workflow management tool, but at the same time, there are some limitations that can make it less attractive nowadays. I have not done a thorough research on this, but there seem to be many alternatives, each one of which has its own strength and weakness.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;R &lt;a href=&#34;https://github.com/ropensci/drake&#34;&gt;drake&lt;/a&gt; package is more R-focused than other pipeline tools, but more importantly, it has a very thorough and informative documentation on such tools in general.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Apache &lt;a href=&#34;https://airflow.apache.org/&#34;&gt;airflow&lt;/a&gt; is “a platform to programmatically author, schedule and monitor workflows.” I really like its ability to visualize dependencies.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Bash script: many data engineers that I know seem to be using bash scripts to do everything, including workflow mangement. It’s certainly a viable option, even when dependencies are considered, especially when you know what you’re doing.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are some great tutorials and documentations on GNU make, and here are a couple more links as a reference.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;GNU Make &lt;a href=&#34;https://www.gnu.org/software/make/manual/make.html&#34;&gt;Manual&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Make Intro by &lt;a href=&#34;http://kbroman.org/minimal_make/&#34;&gt;Karl Broman&lt;/a&gt;, &lt;a href=&#34;https://bost.ocks.org/mike/make/&#34;&gt;Mike Bostock&lt;/a&gt;, &lt;a href=&#34;http://zmjones.com/make/&#34;&gt;Zachary Jones&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Software Carpentry’s &lt;a href=&#34;https://swcarpentry.github.io/make-novice/&#34;&gt;Lesson&lt;/a&gt; on Automation and Make&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Make reads this Makefile (in the current directory) and works on the rules within it.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;I should prob start gathering these topics and write about them sometime in the future.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

        
      </description>
    </item>
    
    <item>
      <title>Home sweet dome</title>
      <link>/post/2018/08/05/home-sweet-dome/</link>
      <pubDate>Sun, 05 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/08/05/home-sweet-dome/</guid>
      <description>
        &lt;p&gt;I ran into a &lt;a href=&#34;https://www.tarheelblog.com/2018/8/4/17644436/unc-basketball-tar-heels-dean-dome-advantage&#34;&gt;post&lt;/a&gt; on the Tar Heel Blog (THB) that talks about the Tar Heel’s home court advantage in recent years. Since it’s been a while since I wrote anything about UNCMBB, I thought it’d be a great topic to write on here too, looking at how great the teams have played on home court&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; in recent years. And maybe I’ll look at how Duke has played on their home court too during the same time period just because.&lt;/p&gt;
&lt;p&gt;THB counted the wins and losses in the past 3 years in particular, and that gave me a starting point.&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Season
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Where
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
wins
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
losses
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2016
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
H
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
H
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
H
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;So it seems my count and their count matches. Then I was curious if this last 3 years have been the best there is, in terms of fewest losses, and for that matter, most wins, and best winning percentage at home. In below charts, season is the starting season of the 3 years. E.g., for season = 2016, value = 4 means, rolling 3 years loss count for 2016, 2017, and 2018 seasons. Championship seasons are represented with larger dots, and the latest 3 year statistics (wins, losses, and winning percentages) with coloured horizontal lines.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-05-home-sweet-dome_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Although this past 3 years loss count (4) is a great feat, it’s not the best there is, it turns out, and there’s been 18 instances with fewer than 4 loss count. Three early championship seasons (1957, 1982, and 1983) stand out with 3 or fewer losses each (again, as the starting season of the next 3 seasons), with the next two championship seasons (2005 and 2009) not so superb loss counts at 6 apiece&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The fewest 3-yr rolling home loss counts came in two separate instances, with a single lose apiece: 1977/1978/1979 seasons and 1978/1979/1980 seasons. Let’s see what those losses were.&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Season
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Game_Date
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Game_Day
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Type
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Where
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Opponent_School
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Result
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Tm
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Opp
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
OT
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1977
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1977-01-26
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Wed
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
REG
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
H
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Wake Forest
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
L
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
66
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
67
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1980
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1980-01-20
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Sun
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
REG
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
H
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Maryland
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
L
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
86
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
92
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Nothing stands out at me right away about Carolina’s 1977 - 1980 teams&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;, let alone 1977 Wake Forest and 1980 Maryland, but it must have been fantastic home court winning streaks for the Tar Heels. Below shows the 3-yr rolling win counts.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-05-home-sweet-dome_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Sure, 1977 to 1979 seasons show great home winning trends at the time (e.g., 35 home wins during 3 seasons starting from 1978 season was the most in UNC history&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; at the time), but the number of games played are somewhat different year after year, so let’s look at the winning percentage instead.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-05-home-sweet-dome_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Yet again, 3 years following 1977 and 1978 seasons showed the highest rolling 3-yr home winning percentages, so it must have been really fun watching Carolina playing and winning at home those days. I feel like I have some homework to do, getting to know them a little better.&lt;/p&gt;
&lt;p&gt;Now that I’ve looked at a couple of statistics for rolling 3 years, I became curious which freshman class has the bragging rights in terms of fewest losses, most wins, and highest winning percentage over their college carrers at home court. This should be pretty straightforward since a typical college career lasts for 4 years, which means all I have to change is the number of rolling years from 3 to 4. Below charts show the rolling 4-yr losses, wins, and win percentages, respectively.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-05-home-sweet-dome_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-05-home-sweet-dome_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-05-home-sweet-dome_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For Carolina, it seems the bragging rights go to the freshman classes in the late 70s whereas for Duke, they could go to any of the teams in the late 80s, early and late 2000s. I had expected Psycho-T’s 2005 freshman class to stand out in any of the three numbers, but to my surprise the class wasn’t the top although they were close especially in terms of rolling 4-yr wins. That got me thinking, maybe they did much better in away games (after all, that class never lost to Duke on their home court during their college careers), and that’s what I’ll be looking at next time.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Which, by the way, is deservingly going to be named after Coach Roy Williams.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Most recent championship season (2017) does not have 3-yr rolling counts yet.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Just a casual fan here :)&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;Since 1949-1950 season to be exact.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

        
      </description>
    </item>
    
    <item>
      <title>My old coding products</title>
      <link>/post/2018/07/27/my-old-coding-products/</link>
      <pubDate>Fri, 27 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/07/27/my-old-coding-products/</guid>
      <description>
        &lt;p&gt;Lately I’ve been thinking about why I care much about everything R and sharing the joy of using R, which deserves its own post. Much of it has to do with how I did and did not get a proper training in coding suitable for data analysis in the past, but as I looked back on my personal coding history, I came across hundreds of code files that I wrote in the past&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. Suffice to say, they were pretty important files in my education history, but I have totally forgotten about them until now.&lt;/p&gt;
&lt;p&gt;They are .m files, meant to be used in Matlab. As I started opening and reading a couple of those files, I had mixed feelings, ranging from agony (“omg, this code was from that stressful point in time”) and embarrassment (“omg, look at the logic and style I was using there”), to delight (“wow I’m defining several functions to be used in main script!”) and motivation (“Yup, I was this bad, and that’s exactly why I care about education in coding (especially R) for data analysis!”).&lt;/p&gt;
&lt;p&gt;It was quite refreshing to see how I used to code in Matlab, which had some similarities with R, hence giving me some perspectives on how my data analysis coding skills might have changed. Here are some observations of the past, in conjunction with present whenever applicable:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Since the purpose of the code files (“program”) was to show my particular engineering method works as intended, the codes were relatively simple having simluated data as its input data. There was no cleaning/exploring step, whereas now, where almost always, there’s no clean data, thus always having to write cleaning/exploring scripts, which is usually done in interactive mode.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Since no cleaning/exploring was needed, my codes seem to be run in batch, not interactively. This is particularly refreshing to me, because I’ve been trying to do more in batch mode at work nowadays, and seems like I was already doing that even back then! But again, that’s because there was no need for heavy use of interactive exploration for simulated data. Also I’m sure I did quite extensive typing in interactive mode back then just to make sure my commands work.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;There was one main script that consists of many lines of codes, in which I used multiple for loops for various combinations of “parameters”. Now that I know a little bit about functional programming and bash programming, I can’t help but wonder how much the codes could have been rewritten (refactored).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I defined several functions, which are called in the main script&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. What was interesting though was that each function was defined and stored in its own file! At first I wondered why I had saved a one liner function in its own file, but seems like that was how it needed to be done back then.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I had one README type file that explains what each file is in that particular “program” (directory). I’m actually quite impressed by this file, because even though the file contains less than 20 lines of texts, it showed I tried to document something for potential users of the codes (probably was for future me!).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For each function, on the otherhand, I didn’t do a good job of describing what each input is supposed to be and do, let alone output. This is somewhat different for me nowadays, especially when I’m working on an R package. Documenting is actually quite enjoyable there, but the problem still occurs when I’m coding for data analysis. There is this tension between keeping scripts vs. writing codes and how much comments to provide, etc.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In some cases, there were hundreds of code files that seemingly differ only by certain part of their file names, indicating I could have benefited from version control and/or better parameterized/modular codes. Again, this seems to be an on-going struggle as to how to write better parameterized/modular codes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I didn’t really have a detailed directory structure. Pretty much everything was under one main “project” directory, and it was hard to distinguish which files were for which setting under the project. This is still relevant, because sometimes I find it hard to decide where to put each “project”: on its own, or under some existing projects?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It was quite a pleasant surprise running into my past code prouducts. I don’t think I’ll ever code in Matlab again, but it was still quite refreshing to learn how I used to do it, especially from the current point of view. I wonder how I’d feel about my current coding in say next 5-10 years!?&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Fortunately, I’ve kept those files in a portable device, which I’m not usually good at doing.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Interestingly, it seems I didn’t have to “source” the files to use the functions. Matlab must have known where to look for the function names.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;E.g., see &lt;a href=&#34;http://www.mathworks.com/help/matlab/matlab_prog/create-functions-in-files.html&#34;&gt;here.&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

        
      </description>
    </item>
    
    <item>
      <title>Comments on data analysis workflow</title>
      <link>/post/2018/07/22/comments-on-data-analysis-workflow/</link>
      <pubDate>Sun, 22 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/07/22/comments-on-data-analysis-workflow/</guid>
      <description>
        &lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;There are several benefits of establishing a good and routine data analysis workflow that you follow on a daily basis. At least two benefits come to mind immediately.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Having a good data analysis workflow is beneficial and needed to do reproducible research/work (RR). RR could mean one thing to one and quite another to others, but to me, doing reproducible work means specifically doing my work in a fashion that allows me to pick up from last touched point in that particular work after some break. For example, after spending 2-3 days on project A, I might go off to do something else for the next 2-3 days, and when I come back to project A, I want to know exactly what’s been done and what I need to do. There are many components of data analysis workflow that can be helpful for one to do RR, and I try to outline them below.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A routine workflow helps us avoid mental fatigue. This may be from one of the books by Hadley, but to me, this means knowing what, where, why, and how to do things right off the bat when starting a new project. Slowly but surely, I’m settling down to a specific approach that I’ll touch upon later, but to get to this point, I tried several different alternatives, and many times ended up failing, for example, to keep track of how to do things (e.g., refresh data with new source data) all over again when needed. Typical challenges have been, but are not limited to, (1) keeping track of data generation, preparation, and movement steps, (2) having to switch between several environments to do specific tasks (e.g., visual inspection), and (3) saving analysis summary and results in a place that’s as close to the scripts as possible.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;history&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;History&lt;/h2&gt;
&lt;p&gt;Next, I outline what I have tried so far with a quick rundown of pros and cons for each, beginning with the earliest approach I tried at my current work.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Local RStudio: suitable for most of my personal needs, I’ve been using RStudio on my local computer for a while before joining current work. And even at current work, RStudio was my first tool of choice.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pros
&lt;ul&gt;
&lt;li&gt;Quick and easy start: fire up an RStudio session (preferably at a Rproj level) and you’re ready for work.&lt;/li&gt;
&lt;li&gt;One stop shop: everything you need, you can do it in RStudio pretty much, such as visuals, Rmarkdown reports, blogging, version control (git), etc.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Cons
&lt;ul&gt;
&lt;li&gt;Privacy and security: the biggest drawback of using RStudio at work is that many times we’re not allowed to download the data to local computer due to privacy and security reasons. This is one single biggest roadblock that ultimately kept me from using it as a tool of choice at work.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Remote RCloud: an in-house analytics and visualization platform, it allows visual inspections, collaboration among data folks, and many other things, such as shiny app. Once RStudio turned out to be no-go for most tasks at work, I turned to RCloud for everything from data loading and summary, back when it was relatively early (yet still stable) in the development stage.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pros
&lt;ul&gt;
&lt;li&gt;Work with company data (small and big): since it’s built in-house, it’s capable of consuming company data.&lt;/li&gt;
&lt;li&gt;Visualization: unlike a remote R shell, visuals in RCloud is just like that in RStudio.&lt;/li&gt;
&lt;li&gt;Cells with not just R, but shell, python, shiny, etc.: RCloud can be used to do work not just in R, but also in python and shell.&lt;/li&gt;
&lt;li&gt;Notebooks: data generation, preparation, plots, and reports can be all saved in a sensible manner.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Cons
&lt;ul&gt;
&lt;li&gt;Connection sometimes lost (usually for long running jobs): mostly in its early days, I experienced RCloud hanging from time to time especially for long running jobs, typically big data ingestion, complex tasks (modeling), etc.&lt;/li&gt;
&lt;li&gt;Need internet/browser: not a biggie, but sometimes need to log in too, and it’s not as quick and easy as starting up a local RStudio session, for example.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Remote R shell: log on to work server, and start up R session!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pros
&lt;ul&gt;
&lt;li&gt;Quick and easy start: just like starting up a local RStudio, starting up a remote R shell is quick and easy, once sshed into remote servers.&lt;/li&gt;
&lt;li&gt;Can work with company data (usually small, but sometimes big too): it’s safe to work with company data in the remote servers, and depending on the need, rather big data can be explored in a remote R session too.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Cons
&lt;ul&gt;
&lt;li&gt;Can’t do visual inspections: one biggest hurdle with this option is its lack of graphic support.&lt;/li&gt;
&lt;li&gt;Many times data come from various sources and need to be prepared in a separate environment (e.g., hive)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pyspark in remote python shell&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pros
&lt;ul&gt;
&lt;li&gt;Scalable solutions for production deliverables&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Cons
&lt;ul&gt;
&lt;li&gt;Spark environment in general fails with error messages that are not easy to back track (i.e., I don’t know what I’m doing wrong)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sparklyr in remote R shell&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pros
&lt;ul&gt;
&lt;li&gt;Scalable solutions in familiar R environment&lt;/li&gt;
&lt;li&gt;Familiar environment (basically R shell)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Cons
&lt;ul&gt;
&lt;li&gt;Unstable? Connection lost frequently (probably because I’m doing something wrong)&lt;/li&gt;
&lt;li&gt;No visualization (same problem as in remote R shell)&lt;/li&gt;
&lt;li&gt;Set up time is “long” compared to non spark R shell&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sparklyr in RCloud&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pros
&lt;ul&gt;
&lt;li&gt;Scalable solutions in familiar R environment&lt;/li&gt;
&lt;li&gt;Visualization&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Cons
&lt;ul&gt;
&lt;li&gt;Big data visualization is not too common (e.g., many times need aggregation/summarization for plotting anyway, which doesn’t need to happen in spark, but in hive)&lt;/li&gt;
&lt;li&gt;Internet/browser&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;current&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Current&lt;/h2&gt;
&lt;p&gt;Before starting to settle down on a particular workflow, I realized that first thing I needed to do was to identify my day-to-day needs. Not exhaustively, answers to below questions may provide better insights to what we do on a daily basis.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What kind of data do I work with the most?&lt;/li&gt;
&lt;li&gt;Where do the data come from?&lt;/li&gt;
&lt;li&gt;What do I do with the data?&lt;/li&gt;
&lt;li&gt;Do I need interactive environment or batch jobs?&lt;/li&gt;
&lt;li&gt;How about ML? Is production ML an important part of my job?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It turns out that more often than not, majority of my day-to-day needs are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explore data stored in hive tables&lt;/li&gt;
&lt;li&gt;Visualize data stored in hive tables&lt;/li&gt;
&lt;li&gt;Summarize/document analysis results&lt;/li&gt;
&lt;li&gt;Train ML models and deliver scores in production&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So it became apparent that the integration of R and hive is rather important in my day-to-day work, and that I still rely much on an interactive environment especially during early stages of a project (aka exploration), which takes indeed the majority of the project time.&lt;/p&gt;
&lt;p&gt;Hence, I needed to come up with a better way to use data stored in hive tables in an interactive R session. After several trials-and-errors, I came to the conclusion that the combination of bash shell scripts, remote R shell, and RCloud should do for majority of what I do during this stage. This workflow uses bash shell scripts and remote R shell for quick data exploration and additional data prep, typically followed by RCloud for visualization and summary/documentation.&lt;/p&gt;
&lt;p&gt;Below, I describe how this workflow typically works.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Create a project directory and start up remote R shell from there&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One screen session and one R shell per project (hence don’t be afraid to have multiple screen sessions running)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Quick/iterative exploration in remote R shell&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data generation and preparation
&lt;ul&gt;
&lt;li&gt;Typically involves (intermediate) hive table generations and loading them in R (hive sql, bash, R shell)&lt;/li&gt;
&lt;li&gt;Use custom bash scripts and internal R packages for common tasks&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Descriptive
&lt;ul&gt;
&lt;li&gt;Typical R operations without visuals&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Additional data prep needed for further actions
&lt;ul&gt;
&lt;li&gt;Typically for visual inspection in RCloud&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Intermediate data saving
&lt;ul&gt;
&lt;li&gt;Typically in RDS format for visual inspections in RCloud&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Iterate above steps as needed&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Visual inspection and summary in RCloud&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a new notebook in RCloud (under a proper higher level directory)&lt;/li&gt;
&lt;li&gt;Start documenting findings from the exploration step&lt;/li&gt;
&lt;li&gt;Visual inspection
&lt;ul&gt;
&lt;li&gt;Using RDS files saved from quick explore step&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Reports and summary
&lt;ul&gt;
&lt;li&gt;Quick notes on findings according to the visuals&lt;/li&gt;
&lt;li&gt;Also quick notes on further actions, if needed&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Further actions typically involve ML&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Remote R shell for POC&lt;/li&gt;
&lt;li&gt;Remote R/python programming&lt;/li&gt;
&lt;li&gt;Scalable solution if needed (Spark)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This workflow is not free of pitfalls of course. Some pros and cons are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Pros&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It allows a quick project start-up and exploration.&lt;/li&gt;
&lt;li&gt;Data generation, prep, movement, and analysis steps are all stored in once file (explore.R).&lt;/li&gt;
&lt;li&gt;Visual inspection and progress/insights summary are all in one place (RCloud).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Cons&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It involves many data writes, which can be costly in terms of time and disk space.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;links&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tidyverse.org/articles/2017/12/workflow-vs-script/&#34; class=&#34;uri&#34;&gt;https://www.tidyverse.org/articles/2017/12/workflow-vs-script/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://r4ds.had.co.nz/explore-intro.html&#34; class=&#34;uri&#34;&gt;http://r4ds.had.co.nz/explore-intro.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://edwinth.github.io/blog/workflow/&#34; class=&#34;uri&#34;&gt;https://edwinth.github.io/blog/workflow/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cacm.acm.org/blogs/blog-cacm/169199-data-science-workflow-overview-and-challenges/fulltext&#34; class=&#34;uri&#34;&gt;https://cacm.acm.org/blogs/blog-cacm/169199-data-science-workflow-overview-and-challenges/fulltext&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

        
      </description>
    </item>
    
    <item>
      <title>External presentation goal</title>
      <link>/post/2018/07/22/external-presentation-goal/</link>
      <pubDate>Sun, 22 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/07/22/external-presentation-goal/</guid>
      <description>
        &lt;p&gt;Like Yihui (of blogdown and many other awesome R packages) whose goal is to publish a book a year&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, I have a similar personal goal that I started last year. While I’d love to write a book a year, it’s too ambitious a goal for me (and many people in general, I’d think) to pursue. Instead, my personal goal is to do an external presentation a year, be it for meetups, conferences, or nearby schools as a guest speaker.&lt;/p&gt;
&lt;p&gt;Back in May, I did a presentation on R package development for the ATL R User group, successfully completing the mission for the year :) The presentation has two decks: (1) main deck (&lt;a href=&#34;https://joongsup.rbind.io/slides/r_pkg_devel_final.pdf&#34;&gt;here&lt;/a&gt;) containing the introduction and high level overview, and (2) reference deck&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; (&lt;a href=&#34;https://joongsup.rbind.io/slides/r_pkg_devel.html&#34;&gt;here&lt;/a&gt;) containing the screenshots of each step of the R package development that I covered during the presentation.&lt;/p&gt;
&lt;p&gt;I should probably add to my personal goal list something about blog posting&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. I’m just happy still that blogging is this easy (again thanks to Yihui!), and so there should be no excuse why I can’t write a post more frequently. Well, I won’t make it an official personal goal just yet, but for now, I will have to keep writing posts whenever I can, sometimes multiple posts at one sitting, just like today! :)&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Amazingly, he’s successfully written a book a year for the last couple of years as far as I know.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;which I talked about &lt;a href=&#34;https://joongsup.rbind.io/post/2018/05/17/insert-images-in-blogdown-post/&#34;&gt;here&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Yihui’s frequent blog posting is also amazing.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

        
      </description>
    </item>
    
  </channel>
</rss>
